__target__: src.models.components.forward_process.NFDM_gaussian

# get input size with ${data.vocab_size} and ${encoder.embedding_dim} and ${encoder.embedding_dim}
# output size shoudl be multiplied by 2 to get the mean and variance.
model:
  __target__: src.models.components.nets.transformers.nano_gpt.GPT
  config:
    block_size: 1024
    vocab_size: ${data.text8.vocab_size} # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: 12
    n_head: 12
    n_embd: 768
    output_size: ${model.components.encoder.embedding_dim} #1536
    dropout: 0.0
    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster

    