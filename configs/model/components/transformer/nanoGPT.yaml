transformer:  
  config:
    block_size: 1024
    vocab_size: ${data.vocab_size} # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: 12
    n_head: 12
    n_embd: 360
    output_size: 720 #1536
    dropout: 0.0
    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster