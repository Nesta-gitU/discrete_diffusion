# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:

  #choose which components to use for the diffusion model
  - /model/components/encoder@model.diffusion.encoder: simple_embedding_encoder.yaml
  - /model/components/decoder@model.diffusion.decoder: similarity_decoder.yaml
  - /model/components/forward_process@model.diffusion.affine: nfdm_gaussain.yaml
  - /model/components/predictor@model.diffusion.pred: predictor.yaml
  - /model/components/volatility@model.diffusion.vol: learned_volatility.yaml

  #choose which backbone to use for the predictor and forward process
  - /model/components/transformer@model.diffusion.affine.model: dit_2x_output.yaml
  - /model/components/transformer@model.diffusion.pred.model: dit.yaml
  
  - override /data: text8
  - override /model: language_diff_module
  - override /callbacks: default
  - override /trainer: gpu
  
 
# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["text8", "dit", "neural_diffusion"]

seed: 12345

trainer:
  limit_train_batches: 2000
  limit_val_batches: 1
  limit_test_batches: 1
  max_epochs: 1

model:
  compute_diffusion_loss: true
  compute_prior_loss: false
  compute_reconstruction_loss: true
  reconstruction_loss_type: diff_anchor
  enable_matmul_tf32: true
  enable_cudnn_tf32: true

logger:
  wandb:
    tags: ${tags}
    group: "text8"
    project: "clean_long_runs"
  aim:
    experiment: "text8test"
