# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
    #choose which components to use for the diffusion model
  - /model/diffusion@model.diffusion: nfdm_static.yaml

  #choose which backbone to use for the predictor and forward process
  - /model/backbone@model.diffusion.pred.model: bert.yaml
  
  - override /data: roc
  - override /model: language_diff_module
  - override /callbacks: default
  - override /trainer: cpu
  
# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["e2e", "dit", "basic_diffusion"]

seed: 12345
#commment
data:
  batch_size: 64
  block_size: 64
  overfit_one_batch: true

callbacks:
  word_sampler:
   log_train_every_n_steps: 25000
   no_epoch_logging: true
  
#one epoch has 12556 batches
#one val epoch has 1396 batches on batch size 256
trainer:
  max_steps: 800000
  check_val_every_n_epoch: 1
  val_check_interval: 50000

model:      
  compute_diffusion_loss: true
  compute_prior_loss: false
  compute_reconstruction_loss: true
  reconstruction_loss_type: collapse
  enable_matmul_tf32: true
  enable_cudnn_tf32: true
  total_steps: ${trainer.max_steps}

logger:
  wandb:
    tags: ${tags}
    group: "roc"
    project: "roc my code" #if just testing garbage use test
    #mode: "disabled" if you dont want ot log a debug run 
  aim:
    experiment: "text8test"
