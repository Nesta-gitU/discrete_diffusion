# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
    #choose which components to use for the diffusion model
  - /model/diffusion@model.diffusion: nfdm.yaml

  #choose which backbone to use for the predictor and forward process
  - /model/backbone@model.diffusion.pred.model: bert.yaml
  - /model/backbone@model.diffusion.affine.model: small_transformer_adalnzero.yaml
  
  - override /data: roc
  - override /model: language_diff_module
  - override /callbacks: default_nfdm
  - override /trainer: gpu
  
# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["bert-base", "roc", "nfdm"]

seed: 12345
#commment
data:
  batch_size: 512 #512
  block_size: 64
  overfit_one_batch: false #should be false normally

callbacks:
  word_sampler:
    log_train_every_n_steps: 100000
    no_epoch_logging: true
    modes: ["star", "marginal"]
    n_steps: 2000
  model_checkpoint:
    every_n_train_steps: 100000 #save every 1000 steps

#one epoch has 12556 batches
#one val epoch has 1396 batches on batch size 256
trainer:
  max_steps: 1600000
  max_epochs: null
  check_val_every_n_epoch: null
  val_check_interval: 50000
  accumulate_grad_batches: 1
  log_every_n_steps: 50
  devices: 1
  strategy: ddp

model:      
  compute_diffusion_loss: x_0
  compute_prior_loss: false #also 0 on gaussian forward
  compute_reconstruction_loss: true
  reconstruction_loss_type: collapse
  enable_matmul_tf32: true
  enable_cudnn_tf32: true
  total_steps: ${trainer.max_steps}
  mask_padding: false
  grad_clipping_type: "warmup"
  grad_clip: 1 # 1cc
  switch_to_rescaled: 300000000
  diffusion:
    diff_loss_type: "elbo" #options are elbo, half_elbo, x_0_prediction
    pred:
      stabilize: true
    affine:
      dont_add_head: True
      model:
        output_dim: 256
        #hidden_dim: 256
  optimizer:
    lr: 1e-4
        #hidden_dim: 512
  time_sampler:
    _target_: src.models.time_samplers.time_samplers.UniformBucketSampler #####below this add stabilization parameters like learning rate warmup and muon 
  do_lr_warmup: true
  use_muon: true
  muon_params:
    use_muon: true
    muon_lr: 0.002
    muon_momentum: 0.95
    muon_ns_steps: 5
    muon_weight_decay: 0.01

logger:
  wandb:
    tags: ${tags}
    group: "roc"
    project: "roc my code" #if just testing garbage use test
    #mode: "disabled" if you dont want ot log a debug run 
  aim:
    experiment: "text8test"

model_name: "roc_nfdm_new2"
restart_from_checkpoint: true
float32_type: high


