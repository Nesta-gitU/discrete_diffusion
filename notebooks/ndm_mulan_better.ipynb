{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d5c0b7-0286-4010-80cf-602f88b930bb",
   "metadata": {},
   "source": [
    "# Reweighted loss functions for the neural flow diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194bd2-3260-478f-9ebf-43524f23b8c3",
   "metadata": {},
   "source": [
    "## Variational diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b480d1c-c11b-44c4-9862-5c15b12cd820",
   "metadata": {},
   "source": [
    "### Definition of the forward process\n",
    "\n",
    "Let's define the forward process\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha x + \\sigma \\varepsilon,\n",
    "    \\quad \\text{where} \\quad\n",
    "    \\alpha^2 + \\sigma^2 = 1\n",
    "\\end{align}\n",
    "\n",
    "Then we have the following connections:\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\frac{z - \\sigma \\varepsilon}{\\alpha}\n",
    "    \\quad \\text{and} \\quad\n",
    "    \\varepsilon = \\frac{z - \\alpha x}{\\sigma}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0acfc7-2fcd-4f5c-8ce6-acb65d6fc807",
   "metadata": {},
   "source": [
    "### Signal-To-Noise Ratio (SNR)\n",
    "\n",
    "Introduce the Signal-To-Noise Ratio (SNR)\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = \\frac{\\alpha^2}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "Reparametrization through the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = e^{-\\gamma}\n",
    "\\end{align}\n",
    "\n",
    "Then we can rewrite the $\\alpha$ and $\\sigma$ coefficients in terms of the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = \\frac{\\alpha^2}{1 - \\alpha^2} = e^{-\\gamma}\n",
    "    \\quad \\Rightarrow \\quad\n",
    "    \\alpha^2 &= \\frac{e^{-\\gamma}}{1 + e^{-\\gamma}} = \\frac{1}{1 + e^{\\gamma}} = \\sigma(-\\gamma) \\\\\n",
    "    \\sigma^2 &= 1 - \\alpha^2 = \\frac{1}{1 + e^{-\\gamma}} = \\sigma(\\gamma)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e88cbe-684b-44a7-81f8-35d5ac45021f",
   "metadata": {},
   "source": [
    "### Conditional ODE and SDEs\n",
    "\n",
    "The conditional ODE is\n",
    "\n",
    "\\begin{align}\n",
    "    f = \\dot{\\alpha} x + \\dot{\\sigma} \\varepsilon = \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x)\n",
    "\\end{align}\n",
    "\n",
    "The conditional score function is\n",
    "\n",
    "\\begin{align}\n",
    "    s = - \\frac{\\varepsilon}{\\sigma} = \\frac{\\alpha x - z}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "Combining the drift of the ODE $f$ and the score function $s$ with the volatility $g$ we can write down the conditional forward SDE\n",
    "\n",
    "\\begin{align}\n",
    "    d z = f^F d t + g d w, \\quad \\text{where} \\quad f^F = f + \\frac{g^2}{2} s\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can write down the conditional backward SDE\n",
    "\n",
    "\\begin{align}\n",
    "    d z = f^B d t + g d \\bar{w}, \\quad \\text{where} \\quad f^B = f - \\frac{g^2}{2} s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b03f8a-f1d1-4624-95c5-22b4d1b84a1b",
   "metadata": {},
   "source": [
    "### Derivation of the volatility\n",
    "\n",
    "In general, the volatility $g$ can be an arbitrary function of time $t$. However, there is one useful consideration that can help us parameterise in a more efficient way.\n",
    "\n",
    "\n",
    "In diffusion models, we aim to match the distribution of trajectories of the forward and reversed processes. The reverse process is Markovian by design. Therefore, to be able to match the distributions of trajectories, the forward process should also be Markovian. TO guaranry this, we can find such a volatility $g$ that makes the forward process independent on $x$.\n",
    "\n",
    "I don't know how to derive $g$ analytically in general case, but we can do it in case of the VDM.\n",
    "\n",
    "\\begin{align}\n",
    "    f^F\n",
    "    &= f + \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x) + \\frac{g^2}{2} \\frac{\\alpha x - z}{\\sigma^2} \\\\\n",
    "    &= \\underbrace{ \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha + \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) }_{=0} x + \\left( \\frac{\\dot{\\sigma}}{\\sigma} - \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z \\\\\n",
    "\\end{align}\n",
    "\n",
    "That gives us the expression for the volatility\n",
    "\n",
    "\\begin{align}\n",
    "    g^2\n",
    "    &= 2 \\frac{\\sigma^2}{\\alpha} \\left( \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\dot{\\alpha} \\right) \\\\\n",
    "    &= 2 \\sigma \\dot{\\sigma} - 2 \\sigma^2 \\frac{\\dot{\\alpha}}{\\alpha} \\\\\n",
    "    &= (\\sigma^2)' - 2 (\\log \\alpha)' \\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "We can also rewrite the volatility in terms of the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    g^2\n",
    "    &= (\\sigma^2)' - 2 (\\log \\alpha)' \\sigma^2 \\\\\n",
    "    &= (\\sigma^2)' - \\frac{2 \\alpha \\dot{\\alpha}}{\\alpha^2} \\sigma^2 \\\\\n",
    "    &= (\\sigma(\\gamma))' - \\frac{(\\sigma(-\\gamma))'}{\\sigma(-\\gamma)} \\sigma(\\gamma) \\\\\n",
    "    &= \\sigma(\\gamma) \\left( 1 - \\sigma(\\gamma) \\right) \\dot{\\gamma} + \\frac{\\sigma(-\\gamma) \\left( 1 - \\sigma(-\\gamma) \\right) \\dot{\\gamma}}{\\sigma(-\\gamma)} \\sigma(\\gamma) \\\\\n",
    "    &= \\sigma(\\gamma) \\dot{\\gamma} \\left( 1 - \\sigma(\\gamma) + \\underbrace{1 - \\sigma(-\\gamma)}_{=\\sigma(\\gamma)} \\right) \\\\\n",
    "    &= \\sigma(\\gamma) \\dot{\\gamma}\n",
    "\\end{align}\n",
    "\n",
    "To keep the volatility function general, but preserve the connection with the gamma function, we derived, we can reperametrize the volatility function as follows\n",
    "\n",
    "\\begin{align}\n",
    "    g^2 = \\sigma(\\gamma) \\dot{\\gamma} \\eta\n",
    "\\end{align}\n",
    "\n",
    "where $\\eta$ is an arbitrary non-negative function of time $t$. If we set $\\eta = 1$, we will recover the Markovian volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c5b85-669b-4ae0-9b3d-8c3d32141352",
   "metadata": {},
   "source": [
    "### Reverse process\n",
    "\n",
    "We define the reverse process through prediction $\\hat{x}(z,t)$ that we substitute into the conditional backward SDE:\n",
    "\n",
    "\\begin{align}\n",
    "    d z = \\hat{f}^B d t + g d \\bar{w}, \\quad \\text{where} \\quad \\hat{f}^B(z, t) = f^B(z, t, \\hat{x}(z,t))\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2500a9755c8e5",
   "metadata": {},
   "source": [
    "### Derivation of the ELBO\n",
    "\n",
    "We know that the ELBO of diffusion models is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_{f^B} \\left\\| f^B - \\hat{f}^B \\right\\|_2^2, \\quad \\text{where} \\quad \\lambda_{f^B} = \\frac{1}{2 g^2}\n",
    "\\end{align}\n",
    "\n",
    "For VDM, we can rewrite the $f^B$ as:\n",
    "\\begin{align}\n",
    "    f^B\n",
    "    &= f - \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x) - \\frac{g^2}{2} \\frac{\\alpha x - z}{\\sigma^2} \\\\\n",
    "    &= \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) x + \\left( \\frac{\\dot{\\sigma}}{\\sigma} + \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z\n",
    "\\end{align}\n",
    "\n",
    "Since the second term doesn't depend on $x$ and will cancel out in the ELBO, we can rewrite the ELBO as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_x \\left\\| x - \\hat{x} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "Let's derive the $\\lambda_x$ coefficient\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_x\n",
    "    &= \\frac{1}{2 g^2} \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\left( \\frac{\\alpha}{2} \\frac{2 \\alpha \\dot{\\alpha}}{\\alpha^2} - \\frac{\\alpha}{2} \\frac{2 \\sigma \\dot{\\sigma}}{\\sigma^2} - \\frac{\\alpha}{2} \\frac{g^2}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\frac{\\alpha^2}{2^2} \\left( \\frac{(\\alpha^2)'}{\\alpha^2} - \\frac{(\\sigma^2)'}{\\sigma^2} - \\frac{g^2}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} \\frac{\\alpha^2}{\\sigma^2} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\frac{(\\alpha^2)'}{\\alpha^2} - \\frac{(\\sigma^2)'}{\\sigma^2} - \\frac{\\sigma^2 \\dot{\\gamma} \\eta}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\frac{\\sigma(-\\gamma) \\left( 1 - \\sigma(-\\gamma) \\right) (-1) \\dot{\\gamma}}{\\sigma(-\\gamma)} - \\frac{\\sigma(\\gamma) \\left( 1 - \\sigma(\\gamma) \\right) \\dot{\\gamma}}{\\sigma(\\gamma)} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\big[ - \\underbrace{\\left( 1 - \\sigma(-\\gamma) \\right)}_{=\\sigma(\\gamma)} -  1 + \\sigma(\\gamma) \\big] \\dot{\\gamma} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( - \\dot{\\gamma} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{ \\dot{\\gamma}^2 \\left( 1 + \\eta \\right)^2 }{2^2 \\dot{\\gamma} \\eta} \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}\n",
    "\n",
    "Since nothing except the last coefficient depends on function $\\eta$, we can see, we can easily find the optimal $\\eta$. It is $\\eta = 1$. Therefore, the optimal volatility function is a Markovian volatility.\n",
    "\n",
    "We can also find a nice connection with the SNR function, when $\\eta = 1$\n",
    "\n",
    "\\begin{align}\n",
    "    SNR' = (e^{-\\gamma})' = -e^{-\\gamma} \\dot{\\gamma}, \\quad \\lambda_x = \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} = - \\frac{1}{2} SNR'\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64913d91c9d4bcac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alternative formulations of the ELBO\n",
    "\n",
    "Similarly, to formulation of the ELBO in terms of the prediction $\\hat{f}^B$ or $\\hat{x}$, we can rewrite the ELBO in terms of prediction of $\\hat{\\varepsilon}$\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\frac{z - \\sigma \\varepsilon}{\\alpha}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_\\varepsilon \\left\\| \\varepsilon - \\hat{\\varepsilon} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_\\varepsilon\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\frac{\\sigma^2}{\\alpha^2} \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} e^{\\gamma} \\\\\n",
    "    &= \\frac{1}{2} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}\n",
    "\n",
    "When $\\eta = 1$, we have\n",
    "\n",
    "\\begin{align}\n",
    "    \\log-SNR' = -\\dot{\\gamma}, \\quad \\lambda_\\varepsilon = \\frac{1}{2} \\dot{\\gamma} = - \\frac{1}{2} \\log-SNR'\n",
    "\\end{align}\n",
    "\n",
    "We can also rewrite the ELBO in terms of the prediction of $\\hat{v}$ function (see Appendix D in [this paper](https://arxiv.org/abs/2202.00512))\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\alpha z - \\sigma v\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_v \\left\\| v - \\hat{v} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_v\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\sigma^2 \\\\\n",
    "    &= \\frac{1}{2} \\frac{e^{-\\gamma}}{1 + e^{-\\gamma}} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\\\\n",
    "    &= \\frac{1}{2} \\sigma(-\\gamma) \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\\\\n",
    "    &= \\frac{1}{2} \\alpha^2 \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0907bbc40101c27",
   "metadata": {},
   "source": [
    "### Reweighted ELBO formulations\n",
    "\n",
    "As we know from a lot of papers, diffusion models often have better performance when trained not with the ELBO objective, but with reweighted ELBO functions like:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_x = \\left\\| x - \\hat{x} \\right\\|_2^2 = \\frac{1}{\\lambda_x} \\mathcal{L} \\quad \\text{or} \\quad \\mathcal{L}_\\varepsilon = \\left\\| \\varepsilon - \\hat{\\varepsilon} \\right\\|_2^2 = \\frac{1}{\\lambda_\\varepsilon} \\mathcal{L}\n",
    "\\end{align}\n",
    "\n",
    "But what should we do if the model predicts $\\hat{x}$ and we want to train a model with $\\mathcal{L}_\\varepsilon$? We can simply take the $\\mathcal{L}_x$ or $\\mathcal{L}$ and reweight it!\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_\\varepsilon = \\frac{\\lambda_x}{\\lambda_\\varepsilon} \\mathcal{L}_x = \\frac{1}{\\lambda_\\varepsilon} \\mathcal{L}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the choice of the reweighting coefficient doesn't depend on the parameterization of the model. We can parameterize the model throgh prediction $\\hat{x}$, $\\hat{\\varepsilon}$, or $\\hat{v}$ with same objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d785e85329dad83",
   "metadata": {},
   "source": [
    "## Rewaighted ELBO for NFDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb40564a218c136",
   "metadata": {},
   "source": [
    "### General case\n",
    "\n",
    "In the general case, when the forward process defined as\n",
    "\n",
    "\\begin{align}\n",
    "    z = F(\\varepsilon, t, x),\n",
    "\\end{align}\n",
    "\n",
    "there is not much we can do. We can rewaight the ELBO with the $\\frac{1}{2 g^2}$ coefficient, which is a part of the ELBO, but I'm not sure if it will help. We can also try to rewaight the ELBO with the different $\\lambda$ coefficients from VDM. However, since $F$ doesn't have any connections with the SNR, I don't now what such a reweighting can give us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d75c770527251f",
   "metadata": {},
   "source": [
    "### Less general case\n",
    "\n",
    "We can consider the case with a less general forward process\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma G(x, t) \\varepsilon\n",
    "\\end{align}\n",
    "\n",
    "This is a Gaussian forward process and it does have a connection with the SNR function. Therefore, we can reweight the ELBO with the $\\lambda$ coefficients from VDM. If $F=x$ and $G=1$, we will recover exactly the VDM case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329cee14dc0fd92",
   "metadata": {},
   "source": [
    "### NDM\n",
    "\n",
    "We can simplify the forward process a bit more and consider the NDM case\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma \\varepsilon\n",
    "\\end{align}\n",
    "\n",
    "For this case, the same logic applies. We can take the true ELBO and reweight it with the $\\lambda$ coefficients from VDM. However, in this case we can even slightly simplify the calculations. Let's write down the ELBO for the NDM:\n",
    "\n",
    "\\begin{align}\n",
    "    f^B\n",
    "    &= f - \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) - \\frac{g^2}{2} \\frac{\\alpha F - z}{\\sigma^2} \\\\\n",
    "    &= \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) F + \\alpha \\dot{F} + \\left( \\frac{\\dot{\\sigma}}{\\sigma} + \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} \n",
    "    &= \\frac{1}{2 g^2} \\left\\| f^B - \\hat{f}^B \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\left\\| \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) \\left( F - \\hat{F} \\right) + \\alpha \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\frac{1}{g} \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{g} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\sqrt{2 \\lambda_x} \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{g} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\sqrt{2 \\lambda_x} \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{\\sigma} \\frac{\\sqrt{\\dot{\\gamma}}}{\\dot{\\gamma}} \\frac{\\sqrt{\\eta}}{\\eta} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| e^{-\\frac{\\gamma}{2}} \\sqrt{\\dot{\\gamma}} \\frac{ 1 + \\eta }{2} \\frac{\\sqrt{\\eta}}{\\eta} \\left( F - \\hat{F} \\right) + e^{-\\frac{\\gamma}{2}} \\frac{\\sqrt{\\dot{\\gamma}}}{\\dot{\\gamma}} \\frac{\\sqrt{\\eta}}{\\eta} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta} \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\lambda_F \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2, \\quad \\text{where} \\quad \\lambda_F = \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, $\\eta = 1$ doesn't necessarily minimises the ELBO in this case.\n",
    "\n",
    "Therefore, if we want to train the NDM with $\\mathcal{L}_x$ objective, we can do it as follows\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_x\n",
    "    &= \\frac{1}{\\lambda_x} \\mathcal{L} \\\\\n",
    "    &= \\frac{\\lambda_F}{\\lambda_x} \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{ \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta} }{ \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} } \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{ 2^2 }{ \\left( 1 + \\eta \\right)^2 } \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7629fb7b031f6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dd6cac4ee22d7",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:28.977970Z",
     "start_time": "2025-03-09T16:41:27.883631Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "ec7382d8d6dbacb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:28.983457Z",
     "start_time": "2025-03-09T16:41:28.980978Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        return self.net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c4f0b3bbf8c13a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.020566Z",
     "start_time": "2025-03-09T16:41:29.017755Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_sde(\n",
    "        sde: Callable[[Tensor, Tensor], tuple[Tensor, Tensor]],\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=False\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps[:-1]):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        f, g = sde(z, t)\n",
    "\n",
    "        w = torch.randn_like(z)\n",
    "        z = z + f * dt + g * w * dt_2\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "25f926f2265269f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.033860Z",
     "start_time": "2025-03-09T16:41:29.025046Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSampler(nn.Module, ABC):\n",
    "    def __init__(self, salt_fraction: Optional[int] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self._salt_fraction = salt_fraction\n",
    "\n",
    "    @abstractmethod\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def loss(self, loss: Tensor, t: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        In terms of minimization of the variance, this loss is not quite correct. Firstly, in lit module,\n",
    "        we detach t and loss. Theoretically we should differentiate end-to-end through loss to obtain\n",
    "        the true gradient w.r.t. parameters of the proposal distribution. However, to do this, we must\n",
    "        differentiate through the training step second time just to optimize the proposal distribution,\n",
    "        which is too expensive. Therefore, we detach t and loss and work with biased gradient. Secondly,\n",
    "        we should take into account the salting, which we don't.\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.prob(t)\n",
    "\n",
    "        l2 = loss ** 2\n",
    "        p2 = p ** 2\n",
    "\n",
    "        return l2 / p2\n",
    "\n",
    "    def forward(self, bs: int) -> tuple[Tensor, Tensor]:\n",
    "        t = self.sample(bs)\n",
    "\n",
    "        dtype = t.dtype\n",
    "        device = t.device\n",
    "\n",
    "        if self._salt_fraction is not None:\n",
    "            assert bs % self._salt_fraction == 0\n",
    "\n",
    "            bs2 = bs // self._salt_fraction\n",
    "            bs1 = bs - bs2\n",
    "\n",
    "            un = D.Uniform(\n",
    "                torch.tensor([0.], dtype=dtype, device=device),\n",
    "                torch.tensor([1.], dtype=dtype, device=device)\n",
    "            )\n",
    "            u = un.sample(torch.Size((bs2,)))\n",
    "\n",
    "            t = torch.cat([t[:bs1], u], dim=0)\n",
    "\n",
    "            p = self.prob(t)\n",
    "\n",
    "            k = 1 / self._salt_fraction\n",
    "            p = p * (1 - k) + k\n",
    "        else:\n",
    "            p = self.prob(t)\n",
    "\n",
    "        return t, p\n",
    "\n",
    "\n",
    "class UniformSampler(TimeSampler):\n",
    "    def __init__(self, salt_fraction: Optional[int] = None):\n",
    "        super().__init__(salt_fraction)\n",
    "\n",
    "        self.register_buffer(\"_l\", torch.tensor(0.))\n",
    "        self.register_buffer(\"_r\", torch.tensor(1.))\n",
    "\n",
    "    @property\n",
    "    def _u(self) -> D.Uniform:\n",
    "        return D.Uniform(self._l, self._r)\n",
    "\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        return self._u.log_prob(t).squeeze(dim=1).exp()\n",
    "\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        return self._u.sample(torch.Size((bs, 1)))\n",
    "\n",
    "\n",
    "class BucketSampler(TimeSampler):\n",
    "    def __init__(self, n: int = 100, salt_fraction: Optional[int] = None):\n",
    "        super().__init__(salt_fraction)\n",
    "\n",
    "        self._logits = nn.Parameter(torch.ones(n))\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _bucket_prob(self) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _bucket_width(self) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def _bucket_height(self) -> Tensor:\n",
    "        return self._bucket_prob / self._bucket_width\n",
    "\n",
    "    @property\n",
    "    def _bucket_bounds(self) -> tuple[Tensor, Tensor]:\n",
    "        w = self._bucket_width\n",
    "\n",
    "        dtype = w.dtype\n",
    "        device = w.device\n",
    "\n",
    "        b_r = torch.cumsum(w, dim=0)\n",
    "        b_l = torch.cat([torch.zeros(1, dtype=dtype, device=device), b_r[:-1]])\n",
    "\n",
    "        return b_l, b_r\n",
    "\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        t = t.flatten()\n",
    "\n",
    "        t, ids_t = torch.sort(t)\n",
    "        inv_ids_t = torch.argsort(ids_t)\n",
    "\n",
    "        b_l, _ = self._bucket_bounds\n",
    "\n",
    "        ids_p = torch.searchsorted(b_l, t, right=True) - 1\n",
    "\n",
    "        p = self._bucket_height\n",
    "        p = torch.index_select(p, 0, ids_p)\n",
    "        p = torch.index_select(p, 0, inv_ids_t)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        b_p = self._bucket_prob\n",
    "        b_l, b_r = self._bucket_bounds\n",
    "\n",
    "        dtype = b_p.dtype\n",
    "        device = b_p.device\n",
    "\n",
    "        cat = D.Categorical(b_p)\n",
    "        ids = cat.sample(torch.Size((bs,)))\n",
    "\n",
    "        un = D.Uniform(\n",
    "            torch.tensor(0., dtype=dtype, device=device),\n",
    "            torch.tensor(1., dtype=dtype, device=device)\n",
    "        )\n",
    "        u = un.sample(torch.Size((bs,)))\n",
    "\n",
    "        t = torch.index_select(b_l, 0, ids) + torch.index_select(b_r - b_l, 0, ids) * u\n",
    "        t = t[:, None]\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "class UniformBucketSampler(BucketSampler):\n",
    "    @property\n",
    "    def _bucket_prob(self) -> Tensor:\n",
    "        logits = self._logits\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "\n",
    "        return torch.softmax(logits, dim=0)\n",
    "\n",
    "    @property\n",
    "    def _bucket_width(self) -> Tensor:\n",
    "        logits = self._logits\n",
    "        dtype = logits.dtype\n",
    "        device = logits.device\n",
    "        n = logits.shape[0]\n",
    "        return torch.ones(n, dtype=dtype, device=device) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f5aff0f729531210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.039891Z",
     "start_time": "2025-03-09T16:41:29.038184Z"
    }
   },
   "outputs": [],
   "source": [
    "def viz_2d_data(data: Tensor):\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=1)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "405b96b0772b1dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.046732Z",
     "start_time": "2025-03-09T16:41:29.044026Z"
    }
   },
   "outputs": [],
   "source": [
    "def viz_2d_path(t_steps: Tensor, path: Tensor, n_lines: int=-1, color: str | None=None):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(15 + path[0, :, 0], path[0, :, 1], s=1)\n",
    "    plt.scatter(path[-1, :, 0], path[-1, :, 1], s=1)\n",
    "    plt.plot(15 * t_steps[:, None] + path[:, :n_lines, 0],\n",
    "             path[:, :n_lines, 1],\n",
    "             color=color, alpha=0.5)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.tick_params(left=False, labelleft=False,\n",
    "                    bottom=False, labelbottom=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217b66a74d47a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.053990Z",
     "start_time": "2025-03-09T16:41:29.051205Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_data(n: int):\n",
    "    scale = 4.\n",
    "    centers = torch.tensor([\n",
    "        [1, 0],\n",
    "        [-1, 0],\n",
    "        [0, 1],\n",
    "        [0, -1],\n",
    "        [1. / np.sqrt(2), 1. / np.sqrt(2)],\n",
    "        [1. / np.sqrt(2), -1. / np.sqrt(2)],\n",
    "        [-1. / np.sqrt(2), 1. / np.sqrt(2)],\n",
    "        [-1. / np.sqrt(2), -1. / np.sqrt(2)]\n",
    "    ], dtype=torch.float32)\n",
    "    centers = scale * centers\n",
    "\n",
    "    x = torch.randn(n, 2)\n",
    "    x = 0.5 * x\n",
    "\n",
    "    center_ids = torch.randint(0, 8, (n,))\n",
    "    x = x + centers[center_ids]\n",
    "\n",
    "    x = x / 2 ** 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "# generates data of dimension 2\n",
    "data = gen_data(10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a62be700e3c11",
   "metadata": {},
   "source": [
    "### Forward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "701ffe4f521dde96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.060240Z",
     "start_time": "2025-03-09T16:41:29.058397Z"
    }
   },
   "outputs": [],
   "source": [
    "def jvp(f, x, v):\n",
    "    return torch.autograd.functional.jvp(\n",
    "        f, x, v,\n",
    "        create_graph=torch.is_grad_enabled()\n",
    "    )\n",
    "\n",
    "\n",
    "def t_dir(f, t):\n",
    "    return jvp(f, t, torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "4af5e7adc5483e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.066589Z",
     "start_time": "2025-03-09T16:41:29.064453Z"
    }
   },
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def get_m_s(self, x: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> tuple[tuple[Tensor, Tensor], tuple[Tensor, Tensor]]:\n",
    "        def f(t_in):\n",
    "            return self.get_m_s(x, t_in)\n",
    "\n",
    "        return t_dir(f, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "8ca89ed17a1ab35c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.074011Z",
     "start_time": "2025-03-09T16:41:29.071185Z"
    }
   },
   "outputs": [],
   "source": [
    "class AffineTransformID(AffineTransform):\n",
    "    @staticmethod\n",
    "    def get_m_s(x, t):\n",
    "        m = x\n",
    "        s = torch.ones_like(x)\n",
    "        return m, s\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, t): # this is VDM. \n",
    "        m, s = AffineTransformID.get_m_s(x, t)\n",
    "\n",
    "        dm = torch.zeros_like(x)\n",
    "        ds = torch.zeros_like(x)\n",
    "\n",
    "        return (m, s), (dm, ds)\n",
    "\n",
    "\n",
    "class AffineTransformHalfNeural(AffineTransform):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(d, d)\n",
    "\n",
    "    def get_m_s(self, x, t):\n",
    "        #x_t = torch.cat([x, t], dim=1)\n",
    "        m = self.net(x)\n",
    "\n",
    "        #m = x + t * m\n",
    "        s = torch.ones_like(x)\n",
    "\n",
    "        return m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a9dc94f03b8d5ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.080854Z",
     "start_time": "2025-03-09T16:41:29.078374Z"
    }
   },
   "outputs": [],
   "source": [
    "class Gamma(nn.Module, ABC):\n",
    "    @staticmethod\n",
    "    def alpha_2(g):\n",
    "        return torch.sigmoid(-g)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigma_2(g):\n",
    "        return torch.sigmoid(g)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_gamma(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, t: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        return t_dir(self.get_gamma, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "2d2407ebefe9fcf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.087510Z",
     "start_time": "2025-03-09T16:41:29.085433Z"
    }
   },
   "outputs": [],
   "source": [
    "class GammaLinear(Gamma):\n",
    "    @staticmethod\n",
    "    def get_gamma(t):\n",
    "        return -10 + 20 * t\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(t):\n",
    "        g = GammaLinear.get_gamma(t)\n",
    "        dg = torch.ones_like(t) * 20\n",
    "        return g, dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "57b352085eead962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.095591Z",
     "start_time": "2025-03-09T16:41:29.091922Z"
    }
   },
   "outputs": [],
   "source": [
    "class PosLinear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        weight = self.sp(self.weight)\n",
    "        bias = self.bias\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "\n",
    "class GammaVDM(Gamma):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = PosLinear(1, 1)\n",
    "        self.fc2 = PosLinear(1, 1024)\n",
    "        self.fc3 = PosLinear(1024, 1)\n",
    "\n",
    "    def get_unnorm_gamma(self, x):\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        y = self.fc2(x)\n",
    "        y = torch.sigmoid(y)\n",
    "        y = self.fc3(y)\n",
    "\n",
    "        return x + y\n",
    "\n",
    "    def get_gamma(self, t):\n",
    "        x_0 = torch.zeros(1, 1)\n",
    "        x_1 = torch.ones(1, 1)\n",
    "        y_0 = torch.ones(1, 1) * (-10)\n",
    "        y_1 = torch.ones(1, 1) * 10 #flag\n",
    "        y_gap = y_1 - y_0\n",
    "\n",
    "        x_adj = torch.cat([x_0, x_1, t], dim=0)\n",
    "        y_adj = self.get_unnorm_gamma(x_adj)\n",
    "        yo_0, yo_1, yo = y_adj[:1], y_adj[1:2], y_adj[2:]\n",
    "\n",
    "        y = y_0 + (y_1 - y_0) * (yo - yo_0) / (yo_1 - yo_0)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "2ec69a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaMuLAN(Gamma):\n",
    "    #implement get_gamma and forward methods\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.gamma_shape = config.gamma_shape\n",
    "        self.n_features = config.seq_len * config.embedding_dim #what does this reflect? -> n flattend features \n",
    "        self.min_gamma = self.config.gamma_min\n",
    "        self.max_minus_min_gamma = self.config.gamma_max - self.config.gamma_min\n",
    "        self.grad_min_epsilon = 0.001\n",
    "\n",
    "\n",
    "\n",
    "        #self.l1 = nn.Linear(self.n_features, self.n_features)\n",
    "        #if I want the noise injection to not depend on the input, more like vdm, since input dependence will be injected through ndm function transformation, I can just make the input of\n",
    "        #l1 to be 1. which means the input is just the time t. That is not fully equal to MuLAN since there the a, b, d are dependent only on input not on t.\n",
    "        #that also mean I will still need to implement the get_gamma method, but not the forward method, we need tdir still. \n",
    "        self.l1 = nn.Linear(2, self.n_features)\n",
    "        self.l2 = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_a = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_b = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_c = nn.Linear(self.n_features, self.n_features)\n",
    "\n",
    "    def _eval_polynomial(self, a, b, c, t):\n",
    "        # Polynomial evaluation\n",
    "        polynomial = (\n",
    "            (a ** 2) * (t ** 5) / 5.0\n",
    "            + (b ** 2 + 2 * a * c) * (t ** 3) / 3.0\n",
    "            + a * b * (t ** 4) / 2.0\n",
    "            + b * c * (t ** 2)\n",
    "            + (c ** 2 + self.grad_min_epsilon) * t)\n",
    "        \n",
    "        scale = ((a ** 2) / 5.0\n",
    "                 + (b ** 2 + 2 * a * c) / 3.0\n",
    "                 + a * b / 2.0\n",
    "                 + b * c\n",
    "                 + (c ** 2 + self.grad_min_epsilon))\n",
    "\n",
    "        return self.min_gamma + self.max_minus_min_gamma * polynomial / scale\n",
    "    \n",
    "    def _grad_t(self, a, b, c, t):\n",
    "        # derivative = (at^2 + bt + c)^2\n",
    "        polynomial = (\n",
    "        (a ** 2) * (t ** 4)\n",
    "        + (b ** 2 + 2 * a * c) * (t ** 2)\n",
    "        + a * b * (t ** 3) * 2.0\n",
    "        + b * c * t * 2\n",
    "        + (c ** 2 + self.grad_min_epsilon))\n",
    "        \n",
    "        scale = ((a ** 2) / 5.0\n",
    "                + (b ** 2 + 2 * a * c) / 3.0\n",
    "                + a * b / 2.0\n",
    "                + b * c\n",
    "                + (c ** 2 + self.grad_min_epsilon))\n",
    "\n",
    "        return self.max_minus_min_gamma * polynomial / scale\n",
    "\n",
    "    def _compute_coefficients(self, x):\n",
    "        _h = torch.nn.functional.silu(self.l1(x))\n",
    "        _h = torch.nn.functional.silu(self.l2(_h))\n",
    "        a = self.l3_a(_h)\n",
    "        b = self.l3_b(_h)\n",
    "        c = 1e-3 + torch.nn.functional.softplus(self.l3_c(_h))\n",
    "        #print(a,b,c)\n",
    "        return a, b, c\n",
    "    \n",
    "    def get_gamma(self, t, x):\n",
    "        a, b, c = self._compute_coefficients(x)\n",
    "        gamma_flat = self._eval_polynomial(a, b, c, t)\n",
    "        #shape should be bs=t.shape[0], gamma_shape\n",
    "        #how do I append a value to the shape though?\n",
    "        \n",
    "        gamma = gamma_flat.view(-1, *self.gamma_shape)\n",
    "        #print(gamma.shape, \"gamma shape\")\n",
    "        return gamma\n",
    "    \n",
    "    def forward(self, t, x):\n",
    "        a, b, c = self._compute_coefficients(x)\n",
    "        dg = self._grad_t(a, b, c, t)\n",
    "        dg = dg.clamp(min=self.grad_min_epsilon)\n",
    "        return self.get_gamma(t, x), dg\n",
    "\n",
    "    #def forward(self, t):\n",
    "    #    gamma, dgamma = t_dir(self.get_gamma, t)\n",
    "    #    #dgamma = torch.clamp(dgamma, min=self.grad_min_epsilon)\n",
    "    #    return gamma, dgamma\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def sigma_2(g):\n",
    "\n",
    "        vector = torch.sigmoid(g)\n",
    "        sigma_2 = torch.diagonal\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "mulan_config = SimpleNamespace(\n",
    "                gamma_shape=(1,),\n",
    "                seq_len= 1,\n",
    "                embedding_dim= 1,\n",
    "                gamma_min= -13.3,\n",
    "                gamma_max= 5.0\n",
    "                )\n",
    "\n",
    "gamma_here = GammaMuLAN(mulan_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5bb1ba44e681c730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.101414Z",
     "start_time": "2025-03-09T16:41:29.099677Z"
    }
   },
   "outputs": [],
   "source": [
    "class VolatilityEta(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "e2a2701951ed0b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.108064Z",
     "start_time": "2025-03-09T16:41:29.105751Z"
    }
   },
   "outputs": [],
   "source": [
    "class VolatilityEtaOne(nn.Module):\n",
    "    def forward(self, t):\n",
    "        return torch.ones_like(t)\n",
    "\n",
    "\n",
    "class VolatilityEtaNeural(nn.Module, ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(1, 1)\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.sp(self.net(t))\n",
    "\n",
    "class VolatilityEtaOneNew(nn.Module):\n",
    "    def forward(self, t):\n",
    "        return torch.ones(t.size(0), 2)\n",
    "#vol = VolatilityEtaOneNew()\n",
    "#print(vol(torch.tensor([1,2,4,5,6,7,54,2,3,4,5,6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab22518a1370b71",
   "metadata": {},
   "source": [
    "### Reverse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "eabb9e1ff1eaabef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.114545Z",
     "start_time": "2025-03-09T16:41:29.112180Z"
    }
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(d + 1, d)\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        z_t = torch.cat([z, t], dim=1)\n",
    "        x = self.net(z_t)\n",
    "\n",
    "        x = (1 - t) * z + (t + 0.01) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416548974191c4d",
   "metadata": {},
   "source": [
    "### Neural diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "de00a7a5911f1cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.122222Z",
     "start_time": "2025-03-09T16:41:29.118767Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralDiffusion(nn.Module):\n",
    "    def __init__(self, transform: AffineTransform, gamma: Gamma, vol_eta: VolatilityEta, pred: Predictor, VAE=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.gamma = gamma\n",
    "        self.vol_eta = vol_eta\n",
    "        self.pred = pred\n",
    "        #self.scalar = nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "        self.VAE = VAE\n",
    "        if VAE:\n",
    "            self.model = Net(2, 4)\n",
    "            print(self.model.requires_grad_, \"model requires grad\")\n",
    "        else:\n",
    "            self.model = Net(2, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x: Tensor, t: Tensor):\n",
    "        #print(\"fakka\")\n",
    "        #print(torch.is_grad_enabled(), \"is grad enabled\")\n",
    "        eps = torch.randn_like(x)\n",
    "\n",
    "        # Check if eps contains NaNs or Infs\n",
    "        if torch.any(torch.isnan(eps)) or torch.any(torch.isinf(eps)):\n",
    "            print(f\"NaN or Inf detected in eps: {eps}\")\n",
    "\n",
    "        if self.VAE:\n",
    "            mean, log_std = self.model(x).chunk(2, dim=1)\n",
    "            std = torch.exp(log_std)\n",
    "            context = mean + std * torch.randn_like(std)\n",
    "        else:\n",
    "            context = self.model(x)\n",
    "\n",
    "        gamma, d_gamma = self.gamma(t, context)\n",
    "\n",
    "        # Check gamma and d_gamma for NaNs and Infs\n",
    "        if torch.any(torch.isnan(gamma)) or torch.any(torch.isinf(gamma)):\n",
    "            print(f\"NaN or Inf detected in gamma: {gamma}\")\n",
    "            print(gamma[torch.isinf(gamma)])\n",
    "            print(gamma[torch.isnan(gamma)])\n",
    "\n",
    "        if torch.any(torch.isnan(d_gamma)) or torch.any(torch.isinf(d_gamma)): # or torch.any(d_gamma < 0.000001):\n",
    "            print(f\"NaN or Inf detected in d_gamma: {d_gamma}\")\n",
    "        \n",
    "        alpha = self.gamma.alpha_2(gamma) ** 0.5\n",
    "        sigma = self.gamma.sigma_2(gamma) ** 0.5\n",
    "\n",
    "        # Check alpha and sigma for NaNs or Infs\n",
    "        if torch.any(torch.isnan(alpha)) or torch.any(torch.isinf(alpha)):\n",
    "            print(f\"NaN or Inf detected in alpha: {alpha}\")\n",
    "        if torch.any(torch.isnan(sigma)) or torch.any(torch.isinf(sigma)):\n",
    "            print(f\"NaN or Inf detected in sigma: {sigma}\")\n",
    "\n",
    "        (m, _), (d_m, _) = self.transform(x, t)\n",
    "\n",
    "        # Check m, d_m for NaNs or Infs\n",
    "        if torch.any(torch.isnan(m)) or torch.any(torch.isinf(m)):\n",
    "            print(f\"NaN or Inf detected in m: {m}\")\n",
    "        if torch.any(torch.isnan(d_m)) or torch.any(torch.isinf(d_m)):\n",
    "            print(f\"NaN or Inf detected in d_m: {d_m}\")\n",
    "\n",
    "        eta = self.vol_eta(t)\n",
    "\n",
    "        # Check eta for NaNs or Infs\n",
    "        if torch.any(torch.isnan(eta)) or torch.any(torch.isinf(eta)):\n",
    "            print(f\"NaN or Inf detected in eta: {eta}\")\n",
    "\n",
    "        z = alpha * m + sigma * eps\n",
    "\n",
    "        # Check z for NaNs or Infs\n",
    "        if torch.any(torch.isnan(z)) or torch.any(torch.isinf(z)):\n",
    "            print(f\"NaN or Inf detected in z: {z}\")\n",
    "\n",
    "        x_ = self.pred(z, t)\n",
    "        #print(x_.requires_grad, \"x_ requires grad\")\n",
    "\n",
    "        # Check x_ for NaNs or Infs\n",
    "        if torch.any(torch.isnan(x_)) or torch.any(torch.isinf(x_)):\n",
    "            print(f\"NaN or Inf detected in x_: {x_}\")\n",
    "\n",
    "        (m_, _), (d_m_, _) = self.transform(x_, t)\n",
    "\n",
    "        # Check m_, d_m_ for NaNs or Infs\n",
    "        if torch.any(torch.isnan(m_)) or torch.any(torch.isinf(m_)):\n",
    "            print(f\"NaN or Inf detected in m_: {m_}\")\n",
    "        if torch.any(torch.isnan(d_m_)) or torch.any(torch.isinf(d_m_)):\n",
    "            print(f\"NaN or Inf detected in d_m_: {d_m_}\")\n",
    "\n",
    "        # ELBO weighting\n",
    "        lmbd = 0.5 * torch.exp(-gamma) * d_gamma / eta\n",
    "        #lmbd = 0.5 * torch.exp(-self.scalar * gamma) * d_gamma / eta\n",
    "        #lmbd = 0.5 * 8 / eta\n",
    "        #print(lmbd)\n",
    "\n",
    "        \n",
    "        #lmbd = 0.5 * d_gamma / eta\n",
    "        #print(\"exp gamma is : \", torch.exp(-gamma))\n",
    "        #print(\"d_gamma is : \", d_gamma)\n",
    "\n",
    "        # Check lmbd for NaNs or Infs\n",
    "        if torch.any(torch.isnan(lmbd)) or torch.any(torch.isinf(lmbd)):\n",
    "            print(f\"NaN or Inf detected in lmbd: {lmbd}\")\n",
    "\n",
    "        # L_x weighting (optional, commented out)\n",
    "        #lmbd = (4 / (1 + eta) ** 2) \n",
    "        #print(\"lmbd is: \", lmbd) #should just be 1, it is\n",
    "        one_over_dgamma = torch.clamp(1 / (d_gamma), max=10000) \n",
    "        if torch.any(one_over_dgamma == 10000):\n",
    "            print(\"clamped\")\n",
    "        \n",
    "        #print(\"one_over_dgamma is: \", one_over_dgamma) #should just be 1, it is\n",
    "\n",
    "        loss = (1 + eta) / 2 * (m - m_) + one_over_dgamma * (d_m - d_m_)\n",
    "\n",
    "        #print(d_gamma[d_gamma < 0.000001], \"dgamma\")\n",
    "        #print(one_over_dgamma[d_gamma < 0.000001])\n",
    "        #print(d_m-d_m_, \"should be zero\")\n",
    "        #print(one_over_dgamma*(d_m - d_m_), \"should be zero\")\n",
    "       \n",
    "        #print(\"unscaled loss is: \", loss)\n",
    "\n",
    "        # Check intermediate loss for NaNs or Infs\n",
    "        if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "            print(f\"NaN or Inf detected in loss before squaring: {loss}\")\n",
    "\n",
    "        loss = loss ** 2\n",
    "\n",
    "        # Check squared loss for NaNs or Infs\n",
    "        if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "            print(f\"NaN or Inf detected in squared loss: {loss}\")\n",
    "\n",
    "        # Stabilises training\n",
    "        #loss_x = (1 + eta) ** 2 / 4 * (x - x_) ** 2\n",
    "\n",
    "        # Check loss_x for NaNs or Infs\n",
    "        #if torch.any(torch.isnan(loss_x)) or torch.any(torch.isinf(loss_x)):\n",
    "        #    print(f\"NaN or Inf detected in loss_x: {loss_x}\")\n",
    "\n",
    "        #loss = 0.5 * loss + 0.5 * loss_x\n",
    "\n",
    "        # Check final loss before applying lambda for NaNs or Infs\n",
    "        if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "            print(f\"NaN or Inf detected in final loss before lambda: {loss}\")\n",
    "\n",
    "        loss = lmbd * loss \n",
    "\n",
    "        # Check final loss after applying lambda for NaNs or Infs\n",
    "        if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "            print(f\"NaN or Inf detected in final loss: {loss}\")\n",
    "\n",
    "        # Optionally, you can add assertions here to ensure that no NaN or Inf values propagate.\n",
    "        assert not torch.any(torch.isnan(loss)), f\"NaN detected in final loss: {loss}\"\n",
    "        assert not torch.any(torch.isinf(loss)), f\"Inf detected in final loss: {loss}\"\n",
    "\n",
    "        loss = loss.sum(dim=1)\n",
    "        #print(loss.shape)\n",
    "\n",
    "        # Check final loss sum for NaNs or Infs\n",
    "        if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "            print(f\"NaN or Inf detected in loss after summing: {loss}\")\n",
    "\n",
    "        \n",
    "        #if we are using VAE we need to also compute the prior VAE loss \n",
    "        if self.VAE:\n",
    "            #print(log_std.shape)\n",
    "            #print(mean.shape)\n",
    "            KLD = -0.5 * torch.sum(1 + (2*log_std) - mean**2 - (2*log_std).exp(), dim=-1)\n",
    "            #print(KLD.shape)\n",
    "            #print(loss.shape)\n",
    "            #print(KLD.requires_grad, \"KLD requires grad\")\n",
    "\n",
    "        #check KLD for NaNs or Infs\n",
    "        if torch.any(torch.isnan(KLD)) or torch.any(torch.isinf(KLD)):\n",
    "            print(f\"NaN or Inf detected in KLD: {KLD}\")\n",
    "            print(KLD)\n",
    "            print(KLD[torch.isinf(KLD)])\n",
    "            print(KLD[torch.isnan(KLD)])\n",
    "            print(KLD[KLD > 10000])\n",
    "        \n",
    "        #print(loss.mean(), \"loss mean\")\n",
    "        return loss, KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc20fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c8916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff5bae11e352252f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391274f4f43b1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.261219Z",
     "start_time": "2025-03-09T16:41:29.126692Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample = gen_data(2 ** 12)\n",
    "\n",
    "viz_2d_data(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "8d977274d1d3cb88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.270692Z",
     "start_time": "2025-03-09T16:41:29.267685Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, time_sampler, gen_data_f):\n",
    "    iter = 10 ** 5\n",
    "    bs = 2 ** 10\n",
    "\n",
    "    #set lr higher for model.gamma than for the rest using param group\n",
    "\n",
    "    optim = torch.optim.Adam([*model.parameters(), *time_sampler.parameters()], lr=1e-2)\n",
    "\n",
    "    pbar = trange(iter)\n",
    "    for i in pbar:\n",
    "        x = gen_data_f(bs)\n",
    "\n",
    "        t, p = time_sampler(bs=bs)\n",
    "        t, p = t.detach(), p.detach()\n",
    "\n",
    "        loss, KLD = model(x, t)\n",
    "        loss = loss + KLD\n",
    "        loss = loss / p + time_sampler.loss(loss.detach(), t)\n",
    "\n",
    "        \n",
    "\n",
    "        # plt.scatter(t[:, 0], loss.detach().numpy(), s=1)\n",
    "        # plt.scatter(t[:, 0], p.detach().numpy(), s=1)\n",
    "        # plt.show()\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            pbar.set_description(f\"{loss.item():.4f}\")\n",
    "\n",
    "        #print(loss)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "a770bbf8afcc9da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:36.632794Z",
     "start_time": "2025-03-09T16:41:29.282810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.requires_grad_ of Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): SELU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SELU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): SELU()\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): SELU()\n",
      "    (8): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")> model requires grad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239.8645:   5%|▌         | 5453/100000 [00:58<16:55, 93.07it/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (100,)) of distribution Categorical(probs: torch.Size([100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m time_sampler \u001b[38;5;241m=\u001b[39m UniformBucketSampler()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#time_sampler = UniformSampler()\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_data_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhallo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[441], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, time_sampler, gen_data_f)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m gen_data_f(bs)\n\u001b[0;32m---> 13\u001b[0m     t, p \u001b[38;5;241m=\u001b[39m \u001b[43mtime_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     t, p \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdetach(), p\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     16\u001b[0m     loss, KLD \u001b[38;5;241m=\u001b[39m model(x, t)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[425], line 33\u001b[0m, in \u001b[0;36mTimeSampler.forward\u001b[0;34m(self, bs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, bs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor]:\n\u001b[0;32m---> 33\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     36\u001b[0m     device \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdevice\n",
      "Cell \u001b[0;32mIn[425], line 135\u001b[0m, in \u001b[0;36mBucketSampler.sample\u001b[0;34m(self, bs)\u001b[0m\n\u001b[1;32m    132\u001b[0m dtype \u001b[38;5;241m=\u001b[39m b_p\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    133\u001b[0m device \u001b[38;5;241m=\u001b[39m b_p\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 135\u001b[0m cat \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m ids \u001b[38;5;241m=\u001b[39m cat\u001b[38;5;241m.\u001b[39msample(torch\u001b[38;5;241m.\u001b[39mSize((bs,)))\n\u001b[1;32m    138\u001b[0m un \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mUniform(\n\u001b[1;32m    139\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m    140\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (100,)) of distribution Categorical(probs: torch.Size([100])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "mulan_config = SimpleNamespace(\n",
    "                gamma_shape=(1,),\n",
    "                seq_len= 1,\n",
    "                embedding_dim= 1,#2\n",
    "                gamma_min= -10, #-13.3\n",
    "                gamma_max= 10, # 5\n",
    "                learn_tau=False,\n",
    "                learn_delta=False\n",
    "            )\n",
    "torch.set_grad_enabled(True)           \n",
    "transform = AffineTransformID()\n",
    "#transform = AffineTransformHalfNeural(d=2)\n",
    "\n",
    "#gamma = GammaLinear()\n",
    "#gamma = GammaVDM()\n",
    "#gamma = GammaBad()\n",
    "gamma = GammaMuLAN(mulan_config)\n",
    "\n",
    "\n",
    "vol_eta = VolatilityEtaOne()\n",
    "#vol_eta = VolatilityEtaNeural()\n",
    "\n",
    "pred = Predictor(d=2)\n",
    "\n",
    "ndm = NeuralDiffusion(transform, gamma, vol_eta, pred, VAE=True)#, VAE=True)\n",
    "ndm.train()\n",
    "\n",
    "time_sampler = UniformBucketSampler()\n",
    "#time_sampler = UniformSampler()\n",
    "\n",
    "train(model=ndm, time_sampler=time_sampler, gen_data_f=gen_data)\n",
    "print(\"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distibution of the VAE latent space\n",
    "def plot_vae_latent_space(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, log_std = model.model(data).chunk(2, dim=1)\n",
    "        std = torch.exp(log_std)\n",
    "        context = mean + std * torch.randn_like(std)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(context[:, 0].numpy(), context[:, 1].numpy(), s=1)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.title(\"VAE Latent Space\")\n",
    "    plt.show()\n",
    "\n",
    "plot_vae_latent_space(ndm, gen_data(2 ** 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(sampler: BucketSampler, num_points=1000):\n",
    "    with torch.no_grad():\n",
    "        # Generate evenly spaced t values\n",
    "        t_values = torch.linspace(0, 1, num_points).view(-1, 1)  # Shape: (num_points, 1)\n",
    "        \n",
    "        # Compute p(t) at these points\n",
    "        p_values = sampler.prob(t_values).cpu().numpy().flatten()\n",
    "        t_values = t_values.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute the integral using the trapezoidal rule\n",
    "        dt = 1 / (num_points - 1)  # Step size\n",
    "        integral = np.trapz(p_values, t_values)  # Numerical integration\n",
    "\n",
    "        # Plot the learned PDF\n",
    "        plt.plot(t_values, p_values, label=f\"Learned PDF (∫p(t)dt ≈ {integral:.4f})\", color=\"b\", linewidth=2)\n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"p(t)\")\n",
    "        plt.title(\"Learned Probability Density Function\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Print the integral value\n",
    "        print(f\"Estimated integral of p(t) over [0,1]: {integral:.4f}\")\n",
    "        if abs(integral - 1) > 0.05:\n",
    "            print(\"⚠️ Warning: PDF is not properly normalized!\")\n",
    "        else:\n",
    "            print(\"✅ PDF is properly normalized.\")\n",
    "\n",
    "plot_pdf(time_sampler)\n",
    "\n",
    "#isnt this the wrong way arround? loss should be higher for high t not low t, and gamma isnt focussed on any particular part yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40946a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    gamma_og = GammaMuLAN(mulan_config)\n",
    "    x = gen_data(300)\n",
    "\n",
    "    #g_og, d_gamma = gamma_og(t, x)\n",
    "\n",
    "    g, _ = gamma(t, x)\n",
    "\n",
    "\n",
    "    plt.plot(t, g)\n",
    "    #plt.plot(t, g_og)\n",
    "    plt.legend([\"Learned\", \"Original\"])\n",
    "    plt.show()\n",
    "\n",
    "    #print(d_gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also print the loss for these same values of 10\n",
    "#x = gen_data(1).repeat(1000, 1)\n",
    "x = gen_data(1000)\n",
    "t_points = torch.linspace(0, 1, 1000)[:, None]\n",
    "\n",
    "losses = []\n",
    "for t in t_points:\n",
    "    t = t.expand(x.shape[0], 1)\n",
    "    #print(t.shape)\n",
    "    #print(x.shape)\n",
    "    #print(t)\n",
    "    loss = ndm(x, t)\n",
    "    avg_loss = loss.mean()\n",
    "    losses.append(avg_loss.item())\n",
    "\n",
    "\n",
    "print(losses)\n",
    "loss = torch.tensor(losses)\n",
    "plt.plot(t_points, loss.detach().numpy())\n",
    "plt.show() \n",
    "\n",
    "#same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1dec6",
   "metadata": {},
   "source": [
    "### Train MuLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74017b13",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "f^B = \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) - \\frac{g^2}{2} \\frac{\\alpha F - z}{\\sigma^2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8700620e4e064",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "### SDE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8a518fd897855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:50.212269Z",
     "start_time": "2025-03-09T16:47:45.548537Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "\n",
    "def sde(z_in, t_in):\n",
    "    \n",
    "    x_ = pred(z_in, t_in)\n",
    "    if ndm.VAE:\n",
    "        context = torch.randn_like(z_in)\n",
    "    else:\n",
    "        context = ndm.model(x_)\n",
    "    \n",
    "    gmm, d_gmm = gamma(t_in, context)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5 #I added this \n",
    "    if torch.any(torch.isnan(gmm)) or torch.any(torch.isinf(gmm)):\n",
    "        print(t_in, \"t_in\")\n",
    "        print(x_, \"x_\")\n",
    "        print(gmm[torch.isnan(gmm)], \"gmm\")\n",
    "        print(\"gmm is nan or inf\")\n",
    "    #print(gmm, \"gmm\")\n",
    "    #print(d_gmm, \"d_gmm\")\n",
    "    #print(alpha_2, \"alpha_2\")\n",
    "    #print(sigma_2, \"sigma_2\")\n",
    "    #print(alpha, \"alpha\")\n",
    "    #print(sigma, \"sigma\")\n",
    "\n",
    "    eta = vol_eta(t_in)\n",
    "\n",
    "    g = (sigma_2 * d_gmm * eta) ** 0.5\n",
    "\n",
    "    (m_, _), (d_m_, _) = transform(x_, t_in)\n",
    "\n",
    "    #print(d_m_, \"d_m\")\n",
    "    #print(sigma_2, \"sigma2\")\n",
    "    #print(eta, \"eta\")\n",
    "    #print(d_gmm, \"d_gmm\")\n",
    "    #print(g, \"g\")\n",
    "\n",
    "    #drift = -alpha * d_gmm * (1 + eta) / 2 * m_ + \\\n",
    "    #        alpha * d_m_ + \\\n",
    "    #        0.5 * d_gmm * (alpha_2 + eta) * z_in\n",
    "    eps = (z_in - alpha * m_) / sigma\n",
    "    alpha_prime = - d_gmm * 0.5 * alpha * (1- alpha_2) \n",
    "    sigma_prime = 0.5 * d_gmm * sigma * (1 - sigma_2)\n",
    "    #dz = -alpha * d_gmm + alpha * d_m_ + sigma * d_gmm * eps\n",
    "    dz = alpha_prime * m_ + alpha * d_m_ + sigma_prime * eps\n",
    "    drift = dz - 0.5 * (g ** 2) * ((alpha * m_ - z_in) / sigma_2)\n",
    "\n",
    "    #print(eps, \"eps\")\n",
    "    #print(alpha_prime, \"alpha_prime\")\n",
    "    #print(sigma_prime, \"sigma_prime\")\n",
    "    #print(dz, \"dz\")\n",
    "    #print(drift, \"drift\")\n",
    "    if torch.any(torch.isnan(drift)) or torch.any(torch.isinf(drift)):\n",
    "        print(\"drift is nan or inf\")\n",
    "    \n",
    "    if torch.any(torch.isnan(g)) or torch.any(torch.isinf(g)):\n",
    "        print(\"g is nan or inf\")\n",
    "\n",
    "    return drift, g\n",
    "\n",
    "\n",
    "_, (t_steps, path) = solve_sde(sde=sde, z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5ebdf",
   "metadata": {},
   "source": [
    "### Star Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e332ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoised_fn(x, t):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7864187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting_part(prev_sample, t, denoised_fn):\n",
    "    def process_xstart(x):\n",
    "        if denoised_fn is not None:\n",
    "            # print(denoised_fn)\n",
    "            x = denoised_fn(x, t)\n",
    "        if False:# clip_denoised:\n",
    "            return x.clamp(-1, 1)\n",
    "        return x\n",
    "    \n",
    "    out = {}\n",
    "    x_ = pred(prev_sample, t) \n",
    "    \n",
    "    x_start = process_xstart(x_)\n",
    "    out[\"pred_xstart\"] = x_start\n",
    "\n",
    "    gmm, _ = gamma(t)\n",
    "    alpha = gamma.alpha_2(gmm) ** 0.5\n",
    "    sigma2 =  gamma.sigma_2(gmm)\n",
    "\n",
    "    m, _ = transform.get_m_s(x_start, t)\n",
    "\n",
    "    out[\"mean\"] = alpha*m\n",
    "    out[\"log_variance\"] = torch.log(sigma2)\n",
    "\n",
    "\n",
    "    noise = torch.randn_like(prev_sample)\n",
    "    nonzero_mask = (\n",
    "        (t != 0).float().view(-1, *([1] * (len(prev_sample.shape) - 1)))\n",
    "    )  # no noise when t == 0\n",
    "    sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise \n",
    "    return sample\n",
    "\n",
    "@torch.no_grad()\n",
    "def discrete_sampling_star(\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=True\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)#[:-1]\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        z = predicting_part(prev_sample=z, t=t, denoised_fn=None)\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path[:-1]))\n",
    "\n",
    "\n",
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "_, (t_steps, path) = discrete_sampling_star(z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390c795",
   "metadata": {},
   "source": [
    "### Correct Marginal Sampling\n",
    "\n",
    "\\begin{align}\n",
    "    z_t = F(\\varepsilon, t, x)\n",
    "\\end{align}\n",
    "\n",
    "Then given z_t and t (n(0,1) noise and t=1), do:\n",
    "\n",
    "1. x_0 prediction, $$x\\_ = model.pred(z_t, t)$$\n",
    "2. get epsilon by inverse big F, $$\\varepsilon = \\frac{(z - \\alpha F) }{\\sigma}$$\n",
    "3. get epsilon s|t, \n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\varepsilon}_{s|t} = \\sqrt{1- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\sigma}_{s|t} = \\sigma_s^2 - \\frac{SNR(t)}{SNR(s)}\\sigma_s^2\n",
    "\\end{align}\n",
    "\n",
    "and,\n",
    "\n",
    "\\begin{align}\n",
    " \\tilde{\\varepsilon} - N(0,1)? \n",
    "\\end{align}\n",
    "because otherwise no new noise would be injected\n",
    "\n",
    "\n",
    "4. get z_s (next z) by feeding through the forward process. \n",
    "\\begin{align}\n",
    "    z = F(\\tilde{\\varepsilon}_{s|t}, s, x)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha_s F + \\sigma_s (\\sqrt{1- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n",
    "but that doesnt match the ndm appendix 1 I think? and it doesnt lead to a marginalized standard deviation of sigma_s, which would instead need,\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha_s F + (\\sqrt{\\sigma_s^2 - \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n",
    "which I suppose you could also write as \n",
    "\\begin{align}\n",
    "    \\tilde{\\varepsilon}_{s|t} = \\frac{1}{\\sigma^2_s}(\\sqrt{\\sigma^2_s- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_marginal(prev_sample, t, s, denoised_fn):\n",
    "    def process_xstart(x):\n",
    "        if denoised_fn is not None:\n",
    "            # print(denoised_fn)\n",
    "            x = denoised_fn(x, t)\n",
    "        if False:# clip_denoised:\n",
    "            return x.clamp(-1, 1)\n",
    "        return x\n",
    "    \n",
    "    #step 1 do prediction \n",
    "    x_ = pred(prev_sample, t) \n",
    "    \n",
    "    x_start = process_xstart(x_)\n",
    "\n",
    "    #step 2 get epsilon\n",
    "    gmm, _ = gamma(t)\n",
    "    alpha2 = gamma.alpha_2(gmm)\n",
    "    sigma2 =  gamma.sigma_2(gmm)\n",
    "    alpha = alpha2 ** 0.5\n",
    "    sigma = sigma2 ** 0.5\n",
    "\n",
    "    m_ , _ = transform.get_m_s(x_start, t)\n",
    "\n",
    "    eps = (prev_sample - alpha * m_) / sigma\n",
    "\n",
    "    #step 3 get epsilon s|t\n",
    "    #we need stepsize for this?\n",
    "    noise = torch.randn_like(prev_sample)\n",
    "    gmm_s, _ = gamma(s)\n",
    "    alpha2_s = gamma.alpha_2(gmm_s)\n",
    "    sigma2_s =  gamma.sigma_2(gmm_s)\n",
    "    alpha_s = alpha2_s ** 0.5\n",
    "    sigma_s = sigma2_s ** 0.5\n",
    "    \n",
    "    m_s , _ = transform.get_m_s(x_start, s)\n",
    "\n",
    "    #print(gmm_s)\n",
    "    snr_t = (alpha2/sigma2).double()\n",
    "    snr_s = (alpha2_s/sigma2_s).double()\n",
    "\n",
    "    #sigma2_tilde_s_t = (1 -  (snr_t / snr_s)).float() #instead of casting back to float here we can alos cast back only after computing epsilon tilde st\n",
    "    sigma2_tilde_s_t = -torch.expm1(gmm_s - gmm) #should be in 0-1\n",
    "    #print(sigma2_tilde_s_t, \"sigma2_tilde_s_t\")\n",
    "\n",
    "    #or option 3\n",
    "\n",
    "    epsilon_tilde_s_t = torch.sqrt(1 - sigma2_tilde_s_t) * eps + (sigma2_tilde_s_t.sqrt()) * noise \n",
    "\n",
    "    #print(\"snr_t\", snr_t[0])\n",
    "    #print(\"snr_s\", snr_s[0])\n",
    "    #print(\"sigma\", sigma2_tilde_s_t) #this should be positive always but isnt so im doing something wrong. \n",
    "\n",
    "    #step 4 get z_s\n",
    "    sample = alpha_s * m_s + sigma_s * epsilon_tilde_s_t\n",
    "    \n",
    "    #if we want to match appendix 1 of ndm paper I think it should instead be\n",
    "    #sample = alpha_s * m_s +  torch.sqrt(sigma2 - sigma2_tilde_s_t) * eps + (sigma2_tilde_s_t ** 0.5) * noise\n",
    "\n",
    "    return sample\n",
    "\n",
    "@torch.no_grad()\n",
    "def discrete_sampling(\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=True\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)#[:-1]\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps[:-1]):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        #I understand I am doing 2x-1 the number of needed forward pass through gamma now, Ill fix that before putting it into the actual code.        \n",
    "        z = get_next_marginal(prev_sample=z, t=t, s=t+dt, denoised_fn=None)\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path))\n",
    "\n",
    "\n",
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "_, (t_steps, path) = discrete_sampling(z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b811cd",
   "metadata": {},
   "source": [
    "### ODE sampling\n",
    "\n",
    "that means we need just f(z,t,eps)|eps=F(z,t,)înverser. We have, \\\\\n",
    "\\begin{align}\n",
    "    z = F(\\varepsilon, t, x),\n",
    "\\end{align}\n",
    "and specifically in the ndm/vdm case, \\\\\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma \\varepsilon\n",
    "\\end{align}\n",
    "The ODE drift is given by \\\\\n",
    "\\begin{align}\n",
    "    dz = f_{tilde}(z_t, t, x\\_)dt\n",
    "\\end{align}\n",
    "Where \n",
    "\\begin{align}\n",
    "    f_{tilde}(z_t, t, x\\_) &= d/dt F(eps, t, x) \\\\\n",
    "    &= alpha' * m + alpha * dm + sigma' * eps\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "f &= \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e16e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "\n",
    "def ode(z_in, t_in):\n",
    "    gmm, d_gmm = gamma(t_in)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5\n",
    "\n",
    "    #eta = vol_eta(t_in)\n",
    "\n",
    "    x_ = pred(z_in, t_in)\n",
    "\n",
    "    (m_, _), (d_m_, _) = transform(x_, t_in)\n",
    "\n",
    "    eps = (z_in - alpha * m_) / sigma\n",
    "    alpha_prime = - d_gmm * 0.5 * alpha * (1- alpha_2) \n",
    "    sigma_prime = 0.5 * d_gmm * sigma * (1 - sigma_2)\n",
    "    #dz = -alpha * d_gmm + alpha * d_m_ + sigma * d_gmm * eps\n",
    "    dz = alpha_prime * m_ + alpha * d_m_ + sigma_prime * eps\n",
    "    #dz = -alpha_prime + alpha * d_m_ + sigma_prime * eps\n",
    "\n",
    "    \n",
    "    return dz, 0\n",
    "\n",
    "\n",
    "_, (t_steps, path) = solve_sde(sde=ode, z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a852b",
   "metadata": {},
   "source": [
    "### Adaptive step size ODE sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "def ode_adaptor(t, z):\n",
    "    #print(t)\n",
    "    t = t.expand(z.shape[0], 1)\n",
    "    #print(z.shape)\n",
    "    #print(t.shape) # bs x 2, why is this bs by 2??, needs to be bs x 1 where t moves from 1 to 0 over time\n",
    "    drift, _ = ode(z, t)\n",
    "    return drift\n",
    "\n",
    "@torch.no_grad()\n",
    "def integrate_ode_torch(z0, t_span=(1.0, 0.0), atol=1e-6, rtol=1e-7):\n",
    "    # Here, we only provide the initial conditions (z0) and the time span (t_span)\n",
    "    # The solver will automatically select adaptive time steps within the provided range\n",
    "    t = torch.linspace(1.0, 0.0, 10000)  # Reverse time to go from 1 to 0\n",
    "    # Solve the ODE using RK45 (dopri5 with adaptive step size)\n",
    "    path = odeint(ode_adaptor, z0, t = t, method='dopri5', atol=atol, rtol=rtol)\n",
    "\n",
    "     # The solver automatically handles the time steps\n",
    "    final_state = path[-1]  # Last step of integration\n",
    "\n",
    "    return final_state, (t, path)\n",
    "\n",
    "bs = 2 ** 12\n",
    "print(bs)\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "final, (t_steps, path) = integrate_ode_torch(z)\n",
    "\n",
    "print(\"steps taken\", len(path))\n",
    "print(\"steps\", t_steps)\n",
    "\n",
    "viz_2d_data(final.detach().numpy())\n",
    "viz_2d_path(t_steps.detach().numpy(), path.detach().numpy(), n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b43e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    g, _ = gamma(t)\n",
    "    alpha2 = gamma.alpha_2(g)\n",
    "    sigma2 = gamma.sigma_2(g)\n",
    "    snr = alpha2 / sigma2\n",
    "\n",
    "    plt.plot(t, snr)\n",
    "    plt.show()\n",
    "\n",
    "#clearly t > s implies snr(t) < snr(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf01e04ee671537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:50.335562Z",
     "start_time": "2025-03-09T16:47:50.236564Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    g, _ = gamma(t)\n",
    "\n",
    "    plt.plot(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a63e0d4787360",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "    gmm, dgamma = gamma(t)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5\n",
    "\n",
    "    #also get alpha and sigma from the sqrt function and plot them in the same graph as the other alpha and sigma, so make three plots\n",
    "\n",
    "    plt.plot(alpha)\n",
    "    plt.legend([\"alpha\", \"alpha_sqrt\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(sigma) #flag2\n",
    "    plt.legend([\"sigma\", \"sigma_sqrt\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(gmm)\n",
    "    plt.legend([\"gamma\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(dgamma)\n",
    "    print(dgamma)\n",
    "    plt.legend([\"dgamma\"])\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
