{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d5c0b7-0286-4010-80cf-602f88b930bb",
   "metadata": {},
   "source": [
    "# Reweighted loss functions for the neural flow diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194bd2-3260-478f-9ebf-43524f23b8c3",
   "metadata": {},
   "source": [
    "## Variational diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b480d1c-c11b-44c4-9862-5c15b12cd820",
   "metadata": {},
   "source": [
    "### Definition of the forward process\n",
    "\n",
    "Let's define the forward process\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha x + \\sigma \\varepsilon,\n",
    "    \\quad \\text{where} \\quad\n",
    "    \\alpha^2 + \\sigma^2 = 1\n",
    "\\end{align}\n",
    "\n",
    "Then we have the following connections:\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\frac{z - \\sigma \\varepsilon}{\\alpha}\n",
    "    \\quad \\text{and} \\quad\n",
    "    \\varepsilon = \\frac{z - \\alpha x}{\\sigma}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0acfc7-2fcd-4f5c-8ce6-acb65d6fc807",
   "metadata": {},
   "source": [
    "### Signal-To-Noise Ratio (SNR)\n",
    "\n",
    "Introduce the Signal-To-Noise Ratio (SNR)\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = \\frac{\\alpha^2}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "Reparametrization through the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = e^{-\\gamma}\n",
    "\\end{align}\n",
    "\n",
    "Then we can rewrite the $\\alpha$ and $\\sigma$ coefficients in terms of the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    SNR = \\frac{\\alpha^2}{1 - \\alpha^2} = e^{-\\gamma}\n",
    "    \\quad \\Rightarrow \\quad\n",
    "    \\alpha^2 &= \\frac{e^{-\\gamma}}{1 + e^{-\\gamma}} = \\frac{1}{1 + e^{\\gamma}} = \\sigma(-\\gamma) \\\\\n",
    "    \\sigma^2 &= 1 - \\alpha^2 = \\frac{1}{1 + e^{-\\gamma}} = \\sigma(\\gamma)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e88cbe-684b-44a7-81f8-35d5ac45021f",
   "metadata": {},
   "source": [
    "### Conditional ODE and SDEs\n",
    "\n",
    "The conditional ODE is\n",
    "\n",
    "\\begin{align}\n",
    "    f = \\dot{\\alpha} x + \\dot{\\sigma} \\varepsilon = \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x)\n",
    "\\end{align}\n",
    "\n",
    "The conditional score function is\n",
    "\n",
    "\\begin{align}\n",
    "    s = - \\frac{\\varepsilon}{\\sigma} = \\frac{\\alpha x - z}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "Combining the drift of the ODE $f$ and the score function $s$ with the volatility $g$ we can write down the conditional forward SDE\n",
    "\n",
    "\\begin{align}\n",
    "    d z = f^F d t + g d w, \\quad \\text{where} \\quad f^F = f + \\frac{g^2}{2} s\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can write down the conditional backward SDE\n",
    "\n",
    "\\begin{align}\n",
    "    d z = f^B d t + g d \\bar{w}, \\quad \\text{where} \\quad f^B = f - \\frac{g^2}{2} s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b03f8a-f1d1-4624-95c5-22b4d1b84a1b",
   "metadata": {},
   "source": [
    "### Derivation of the volatility\n",
    "\n",
    "In general, the volatility $g$ can be an arbitrary function of time $t$. However, there is one useful consideration that can help us parameterise in a more efficient way.\n",
    "\n",
    "\n",
    "In diffusion models, we aim to match the distribution of trajectories of the forward and reversed processes. The reverse process is Markovian by design. Therefore, to be able to match the distributions of trajectories, the forward process should also be Markovian. TO guaranry this, we can find such a volatility $g$ that makes the forward process independent on $x$.\n",
    "\n",
    "I don't know how to derive $g$ analytically in general case, but we can do it in case of the VDM.\n",
    "\n",
    "\\begin{align}\n",
    "    f^F\n",
    "    &= f + \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x) + \\frac{g^2}{2} \\frac{\\alpha x - z}{\\sigma^2} \\\\\n",
    "    &= \\underbrace{ \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha + \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) }_{=0} x + \\left( \\frac{\\dot{\\sigma}}{\\sigma} - \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z \\\\\n",
    "\\end{align}\n",
    "\n",
    "That gives us the expression for the volatility\n",
    "\n",
    "\\begin{align}\n",
    "    g^2\n",
    "    &= 2 \\frac{\\sigma^2}{\\alpha} \\left( \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\dot{\\alpha} \\right) \\\\\n",
    "    &= 2 \\sigma \\dot{\\sigma} - 2 \\sigma^2 \\frac{\\dot{\\alpha}}{\\alpha} \\\\\n",
    "    &= (\\sigma^2)' - 2 (\\log \\alpha)' \\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "We can also rewrite the volatility in terms of the gamma function\n",
    "\n",
    "\\begin{align}\n",
    "    g^2\n",
    "    &= (\\sigma^2)' - 2 (\\log \\alpha)' \\sigma^2 \\\\\n",
    "    &= (\\sigma^2)' - \\frac{2 \\alpha \\dot{\\alpha}}{\\alpha^2} \\sigma^2 \\\\\n",
    "    &= (\\sigma(\\gamma))' - \\frac{(\\sigma(-\\gamma))'}{\\sigma(-\\gamma)} \\sigma(\\gamma) \\\\\n",
    "    &= \\sigma(\\gamma) \\left( 1 - \\sigma(\\gamma) \\right) \\dot{\\gamma} + \\frac{\\sigma(-\\gamma) \\left( 1 - \\sigma(-\\gamma) \\right) \\dot{\\gamma}}{\\sigma(-\\gamma)} \\sigma(\\gamma) \\\\\n",
    "    &= \\sigma(\\gamma) \\dot{\\gamma} \\left( 1 - \\sigma(\\gamma) + \\underbrace{1 - \\sigma(-\\gamma)}_{=\\sigma(\\gamma)} \\right) \\\\\n",
    "    &= \\sigma(\\gamma) \\dot{\\gamma}\n",
    "\\end{align}\n",
    "\n",
    "To keep the volatility function general, but preserve the connection with the gamma function, we derived, we can reperametrize the volatility function as follows\n",
    "\n",
    "\\begin{align}\n",
    "    g^2 = \\sigma(\\gamma) \\dot{\\gamma} \\eta\n",
    "\\end{align}\n",
    "\n",
    "where $\\eta$ is an arbitrary non-negative function of time $t$. If we set $\\eta = 1$, we will recover the Markovian volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c5b85-669b-4ae0-9b3d-8c3d32141352",
   "metadata": {},
   "source": [
    "### Reverse process\n",
    "\n",
    "We define the reverse process through prediction $\\hat{x}(z,t)$ that we substitute into the conditional backward SDE:\n",
    "\n",
    "\\begin{align}\n",
    "    d z = \\hat{f}^B d t + g d \\bar{w}, \\quad \\text{where} \\quad \\hat{f}^B(z, t) = f^B(z, t, \\hat{x}(z,t))\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2500a9755c8e5",
   "metadata": {},
   "source": [
    "### Derivation of the ELBO\n",
    "\n",
    "We know that the ELBO of diffusion models is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_{f^B} \\left\\| f^B - \\hat{f}^B \\right\\|_2^2, \\quad \\text{where} \\quad \\lambda_{f^B} = \\frac{1}{2 g^2}\n",
    "\\end{align}\n",
    "\n",
    "For VDM, we can rewrite the $f^B$ as:\n",
    "\\begin{align}\n",
    "    f^B\n",
    "    &= f - \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} x + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha x) - \\frac{g^2}{2} \\frac{\\alpha x - z}{\\sigma^2} \\\\\n",
    "    &= \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) x + \\left( \\frac{\\dot{\\sigma}}{\\sigma} + \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z\n",
    "\\end{align}\n",
    "\n",
    "Since the second term doesn't depend on $x$ and will cancel out in the ELBO, we can rewrite the ELBO as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_x \\left\\| x - \\hat{x} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "Let's derive the $\\lambda_x$ coefficient\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_x\n",
    "    &= \\frac{1}{2 g^2} \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\left( \\frac{\\alpha}{2} \\frac{2 \\alpha \\dot{\\alpha}}{\\alpha^2} - \\frac{\\alpha}{2} \\frac{2 \\sigma \\dot{\\sigma}}{\\sigma^2} - \\frac{\\alpha}{2} \\frac{g^2}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\frac{\\alpha^2}{2^2} \\left( \\frac{(\\alpha^2)'}{\\alpha^2} - \\frac{(\\sigma^2)'}{\\sigma^2} - \\frac{g^2}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} \\frac{\\alpha^2}{\\sigma^2} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\frac{(\\alpha^2)'}{\\alpha^2} - \\frac{(\\sigma^2)'}{\\sigma^2} - \\frac{\\sigma^2 \\dot{\\gamma} \\eta}{\\sigma^2} \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\frac{\\sigma(-\\gamma) \\left( 1 - \\sigma(-\\gamma) \\right) (-1) \\dot{\\gamma}}{\\sigma(-\\gamma)} - \\frac{\\sigma(\\gamma) \\left( 1 - \\sigma(\\gamma) \\right) \\dot{\\gamma}}{\\sigma(\\gamma)} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( \\big[ - \\underbrace{\\left( 1 - \\sigma(-\\gamma) \\right)}_{=\\sigma(\\gamma)} -  1 + \\sigma(\\gamma) \\big] \\dot{\\gamma} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{1}{2^2 \\dot{\\gamma} \\eta} \\left( - \\dot{\\gamma} - \\dot{\\gamma} \\eta \\right)^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\frac{ \\dot{\\gamma}^2 \\left( 1 + \\eta \\right)^2 }{2^2 \\dot{\\gamma} \\eta} \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}\n",
    "\n",
    "Since nothing except the last coefficient depends on function $\\eta$, we can see, we can easily find the optimal $\\eta$. It is $\\eta = 1$. Therefore, the optimal volatility function is a Markovian volatility.\n",
    "\n",
    "We can also find a nice connection with the SNR function, when $\\eta = 1$\n",
    "\n",
    "\\begin{align}\n",
    "    SNR' = (e^{-\\gamma})' = -e^{-\\gamma} \\dot{\\gamma}, \\quad \\lambda_x = \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} = - \\frac{1}{2} SNR'\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64913d91c9d4bcac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alternative formulations of the ELBO\n",
    "\n",
    "Similarly, to formulation of the ELBO in terms of the prediction $\\hat{f}^B$ or $\\hat{x}$, we can rewrite the ELBO in terms of prediction of $\\hat{\\varepsilon}$\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\frac{z - \\sigma \\varepsilon}{\\alpha}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_\\varepsilon \\left\\| \\varepsilon - \\hat{\\varepsilon} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_\\varepsilon\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\frac{\\sigma^2}{\\alpha^2} \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} e^{\\gamma} \\\\\n",
    "    &= \\frac{1}{2} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}\n",
    "\n",
    "When $\\eta = 1$, we have\n",
    "\n",
    "\\begin{align}\n",
    "    \\log-SNR' = -\\dot{\\gamma}, \\quad \\lambda_\\varepsilon = \\frac{1}{2} \\dot{\\gamma} = - \\frac{1}{2} \\log-SNR'\n",
    "\\end{align}\n",
    "\n",
    "We can also rewrite the ELBO in terms of the prediction of $\\hat{v}$ function (see Appendix D in [this paper](https://arxiv.org/abs/2202.00512))\n",
    "\n",
    "\\begin{align}\n",
    "    x = \\alpha z - \\sigma v\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = \\lambda_v \\left\\| v - \\hat{v} \\right\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_v\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\sigma^2 \\\\\n",
    "    &= \\frac{1}{2} \\frac{e^{-\\gamma}}{1 + e^{-\\gamma}} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\\\\n",
    "    &= \\frac{1}{2} \\sigma(-\\gamma) \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} \\\\\n",
    "    &= \\frac{1}{2} \\alpha^2 \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0907bbc40101c27",
   "metadata": {},
   "source": [
    "### Reweighted ELBO formulations\n",
    "\n",
    "As we know from a lot of papers, diffusion models often have better performance when trained not with the ELBO objective, but with reweighted ELBO functions like:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_x = \\left\\| x - \\hat{x} \\right\\|_2^2 = \\frac{1}{\\lambda_x} \\mathcal{L} \\quad \\text{or} \\quad \\mathcal{L}_\\varepsilon = \\left\\| \\varepsilon - \\hat{\\varepsilon} \\right\\|_2^2 = \\frac{1}{\\lambda_\\varepsilon} \\mathcal{L}\n",
    "\\end{align}\n",
    "\n",
    "But what should we do if the model predicts $\\hat{x}$ and we want to train a model with $\\mathcal{L}_\\varepsilon$? We can simply take the $\\mathcal{L}_x$ or $\\mathcal{L}$ and reweight it!\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_\\varepsilon = \\frac{\\lambda_x}{\\lambda_\\varepsilon} \\mathcal{L}_x = \\frac{1}{\\lambda_\\varepsilon} \\mathcal{L}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, the choice of the reweighting coefficient doesn't depend on the parameterization of the model. We can parameterize the model throgh prediction $\\hat{x}$, $\\hat{\\varepsilon}$, or $\\hat{v}$ with same objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d785e85329dad83",
   "metadata": {},
   "source": [
    "## Rewaighted ELBO for NFDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb40564a218c136",
   "metadata": {},
   "source": [
    "### General case\n",
    "\n",
    "In the general case, when the forward process defined as\n",
    "\n",
    "\\begin{align}\n",
    "    z = F(\\varepsilon, t, x),\n",
    "\\end{align}\n",
    "\n",
    "there is not much we can do. We can rewaight the ELBO with the $\\frac{1}{2 g^2}$ coefficient, which is a part of the ELBO, but I'm not sure if it will help. We can also try to rewaight the ELBO with the different $\\lambda$ coefficients from VDM. However, since $F$ doesn't have any connections with the SNR, I don't now what such a reweighting can give us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d75c770527251f",
   "metadata": {},
   "source": [
    "### Less general case\n",
    "\n",
    "We can consider the case with a less general forward process\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma G(x, t) \\varepsilon\n",
    "\\end{align}\n",
    "\n",
    "This is a Gaussian forward process and it does have a connection with the SNR function. Therefore, we can reweight the ELBO with the $\\lambda$ coefficients from VDM. If $F=x$ and $G=1$, we will recover exactly the VDM case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329cee14dc0fd92",
   "metadata": {},
   "source": [
    "### NDM\n",
    "\n",
    "We can simplify the forward process a bit more and consider the NDM case\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma \\varepsilon\n",
    "\\end{align}\n",
    "\n",
    "For this case, the same logic applies. We can take the true ELBO and reweight it with the $\\lambda$ coefficients from VDM. However, in this case we can even slightly simplify the calculations. Let's write down the ELBO for the NDM:\n",
    "\n",
    "\\begin{align}\n",
    "    f^B\n",
    "    &= f - \\frac{g^2}{2} s \\\\\n",
    "    &= \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) - \\frac{g^2}{2} \\frac{\\alpha F - z}{\\sigma^2} \\\\\n",
    "    &= \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) F + \\alpha \\dot{F} + \\left( \\frac{\\dot{\\sigma}}{\\sigma} + \\frac{g^2}{2} \\frac{1}{\\sigma^2} \\right) z\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} \n",
    "    &= \\frac{1}{2 g^2} \\left\\| f^B - \\hat{f}^B \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2 g^2} \\left\\| \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) \\left( F - \\hat{F} \\right) + \\alpha \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\frac{1}{g} \\left( \\dot{\\alpha} - \\frac{\\dot{\\sigma}}{\\sigma} \\alpha - \\frac{g^2}{2} \\frac{\\alpha}{\\sigma^2} \\right) \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{g} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\sqrt{2 \\lambda_x} \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{g} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| \\sqrt{2 \\lambda_x} \\left( F - \\hat{F} \\right) + \\frac{\\alpha}{\\sigma} \\frac{\\sqrt{\\dot{\\gamma}}}{\\dot{\\gamma}} \\frac{\\sqrt{\\eta}}{\\eta} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} \\left\\| e^{-\\frac{\\gamma}{2}} \\sqrt{\\dot{\\gamma}} \\frac{ 1 + \\eta }{2} \\frac{\\sqrt{\\eta}}{\\eta} \\left( F - \\hat{F} \\right) + e^{-\\frac{\\gamma}{2}} \\frac{\\sqrt{\\dot{\\gamma}}}{\\dot{\\gamma}} \\frac{\\sqrt{\\eta}}{\\eta} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta} \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\lambda_F \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2, \\quad \\text{where} \\quad \\lambda_F = \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta}\n",
    "\\end{align}\n",
    "\n",
    "Importantly, $\\eta = 1$ doesn't necessarily minimises the ELBO in this case.\n",
    "\n",
    "Therefore, if we want to train the NDM with $\\mathcal{L}_x$ objective, we can do it as follows\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_x\n",
    "    &= \\frac{1}{\\lambda_x} \\mathcal{L} \\\\\n",
    "    &= \\frac{\\lambda_F}{\\lambda_x} \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{ \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{1}{\\eta} }{ \\frac{1}{2} e^{-\\gamma} \\dot{\\gamma} \\frac{ \\left( 1 + \\eta \\right)^2 }{2^2 \\eta} } \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2 \\\\\n",
    "    &= \\frac{ 2^2 }{ \\left( 1 + \\eta \\right)^2 } \\left\\| \\frac{ 1 + \\eta }{2} \\left( F - \\hat{F} \\right) + \\frac{1}{\\dot{\\gamma}} \\left( \\dot{F} - \\dot{\\hat{F}} \\right) \\right\\|_2^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7629fb7b031f6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dd6cac4ee22d7",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:28.977970Z",
     "start_time": "2025-03-09T16:41:27.883631Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec7382d8d6dbacb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:28.983457Z",
     "start_time": "2025-03-09T16:41:28.980978Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        return self.net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4f0b3bbf8c13a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.020566Z",
     "start_time": "2025-03-09T16:41:29.017755Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_sde(\n",
    "        sde: Callable[[Tensor, Tensor], tuple[Tensor, Tensor]],\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=False\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps[:-1]):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        f, g = sde(z, t)\n",
    "\n",
    "        w = torch.randn_like(z)\n",
    "        z = z + f * dt + g * w * dt_2\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f926f2265269f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.033860Z",
     "start_time": "2025-03-09T16:41:29.025046Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSampler(nn.Module, ABC):\n",
    "    def __init__(self, salt_fraction: Optional[int] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self._salt_fraction = salt_fraction\n",
    "\n",
    "    @abstractmethod\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def loss(self, loss: Tensor, t: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        In terms of minimization of the variance, this loss is not quite correct. Firstly, in lit module,\n",
    "        we detach t and loss. Theoretically we should differentiate end-to-end through loss to obtain\n",
    "        the true gradient w.r.t. parameters of the proposal distribution. However, to do this, we must\n",
    "        differentiate through the training step second time just to optimize the proposal distribution,\n",
    "        which is too expensive. Therefore, we detach t and loss and work with biased gradient. Secondly,\n",
    "        we should take into account the salting, which we don't.\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.prob(t)\n",
    "\n",
    "        l2 = loss ** 2\n",
    "        p2 = p ** 2\n",
    "\n",
    "        return l2 / p2\n",
    "\n",
    "    def forward(self, bs: int) -> tuple[Tensor, Tensor]:\n",
    "        t = self.sample(bs)\n",
    "\n",
    "        dtype = t.dtype\n",
    "        device = t.device\n",
    "\n",
    "        if self._salt_fraction is not None:\n",
    "            assert bs % self._salt_fraction == 0\n",
    "\n",
    "            bs2 = bs // self._salt_fraction\n",
    "            bs1 = bs - bs2\n",
    "\n",
    "            un = D.Uniform(\n",
    "                torch.tensor([0.], dtype=dtype, device=device),\n",
    "                torch.tensor([1.], dtype=dtype, device=device)\n",
    "            )\n",
    "            u = un.sample(torch.Size((bs2,)))\n",
    "\n",
    "            t = torch.cat([t[:bs1], u], dim=0)\n",
    "\n",
    "            p = self.prob(t)\n",
    "\n",
    "            k = 1 / self._salt_fraction\n",
    "            p = p * (1 - k) + k\n",
    "        else:\n",
    "            p = self.prob(t)\n",
    "\n",
    "        return t, p\n",
    "\n",
    "\n",
    "class UniformSampler(TimeSampler):\n",
    "    def __init__(self, salt_fraction: Optional[int] = None):\n",
    "        super().__init__(salt_fraction)\n",
    "\n",
    "        self.register_buffer(\"_l\", torch.tensor(0.))\n",
    "        self.register_buffer(\"_r\", torch.tensor(1.))\n",
    "\n",
    "    @property\n",
    "    def _u(self) -> D.Uniform:\n",
    "        return D.Uniform(self._l, self._r)\n",
    "\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        return self._u.log_prob(t).squeeze(dim=1).exp()\n",
    "\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        return self._u.sample(torch.Size((bs, 1)))\n",
    "\n",
    "\n",
    "class BucketSampler(TimeSampler):\n",
    "    def __init__(self, n: int = 100, salt_fraction: Optional[int] = None):\n",
    "        super().__init__(salt_fraction)\n",
    "\n",
    "        self._logits = nn.Parameter(torch.ones(n))\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _bucket_prob(self) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _bucket_width(self) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def _bucket_height(self) -> Tensor:\n",
    "        return self._bucket_prob / self._bucket_width\n",
    "\n",
    "    @property\n",
    "    def _bucket_bounds(self) -> tuple[Tensor, Tensor]:\n",
    "        w = self._bucket_width\n",
    "\n",
    "        dtype = w.dtype\n",
    "        device = w.device\n",
    "\n",
    "        b_r = torch.cumsum(w, dim=0)\n",
    "        b_l = torch.cat([torch.zeros(1, dtype=dtype, device=device), b_r[:-1]])\n",
    "\n",
    "        return b_l, b_r\n",
    "\n",
    "    def prob(self, t: Tensor) -> Tensor:\n",
    "        t = t.flatten()\n",
    "\n",
    "        t, ids_t = torch.sort(t)\n",
    "        inv_ids_t = torch.argsort(ids_t)\n",
    "\n",
    "        b_l, _ = self._bucket_bounds\n",
    "\n",
    "        ids_p = torch.searchsorted(b_l, t, right=True) - 1\n",
    "\n",
    "        p = self._bucket_height\n",
    "        p = torch.index_select(p, 0, ids_p)\n",
    "        p = torch.index_select(p, 0, inv_ids_t)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def sample(self, bs: int) -> Tensor:\n",
    "        b_p = self._bucket_prob\n",
    "        b_l, b_r = self._bucket_bounds\n",
    "\n",
    "        dtype = b_p.dtype\n",
    "        device = b_p.device\n",
    "\n",
    "        cat = D.Categorical(b_p)\n",
    "        ids = cat.sample(torch.Size((bs,)))\n",
    "\n",
    "        un = D.Uniform(\n",
    "            torch.tensor(0., dtype=dtype, device=device),\n",
    "            torch.tensor(1., dtype=dtype, device=device)\n",
    "        )\n",
    "        u = un.sample(torch.Size((bs,)))\n",
    "\n",
    "        t = torch.index_select(b_l, 0, ids) + torch.index_select(b_r - b_l, 0, ids) * u\n",
    "        t = t[:, None]\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "class UniformBucketSampler(BucketSampler):\n",
    "    @property\n",
    "    def _bucket_prob(self) -> Tensor:\n",
    "        logits = self._logits\n",
    "        logits = torch.clamp(logits, min=-10, max=10)\n",
    "\n",
    "        return torch.softmax(logits, dim=0)\n",
    "\n",
    "    @property\n",
    "    def _bucket_width(self) -> Tensor:\n",
    "        logits = self._logits\n",
    "        dtype = logits.dtype\n",
    "        device = logits.device\n",
    "        n = logits.shape[0]\n",
    "        return torch.ones(n, dtype=dtype, device=device) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5aff0f729531210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.039891Z",
     "start_time": "2025-03-09T16:41:29.038184Z"
    }
   },
   "outputs": [],
   "source": [
    "def viz_2d_data(data: Tensor):\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=1)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "405b96b0772b1dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.046732Z",
     "start_time": "2025-03-09T16:41:29.044026Z"
    }
   },
   "outputs": [],
   "source": [
    "def viz_2d_path(t_steps: Tensor, path: Tensor, n_lines: int=-1, color: str | None=None):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(15 + path[0, :, 0], path[0, :, 1], s=1)\n",
    "    plt.scatter(path[-1, :, 0], path[-1, :, 1], s=1)\n",
    "    plt.plot(15 * t_steps[:, None] + path[:, :n_lines, 0],\n",
    "             path[:, :n_lines, 1],\n",
    "             color=color, alpha=0.5)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.tick_params(left=False, labelleft=False,\n",
    "                    bottom=False, labelbottom=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d217b66a74d47a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.053990Z",
     "start_time": "2025-03-09T16:41:29.051205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3416,  2.4032],\n",
      "        [ 1.2083,  2.2595],\n",
      "        [-1.4214,  1.8108],\n",
      "        [-0.3601, -2.4644],\n",
      "        [ 2.0731, -1.2966],\n",
      "        [-2.0481, -2.5009],\n",
      "        [ 3.1622,  0.6033],\n",
      "        [-1.5978,  2.2479],\n",
      "        [ 1.3356,  1.8097],\n",
      "        [-3.0082, -0.1457]])\n"
     ]
    }
   ],
   "source": [
    "def gen_data(n: int):\n",
    "    scale = 4.\n",
    "    centers = torch.tensor([\n",
    "        [1, 0],\n",
    "        [-1, 0],\n",
    "        [0, 1],\n",
    "        [0, -1],\n",
    "        [1. / np.sqrt(2), 1. / np.sqrt(2)],\n",
    "        [1. / np.sqrt(2), -1. / np.sqrt(2)],\n",
    "        [-1. / np.sqrt(2), 1. / np.sqrt(2)],\n",
    "        [-1. / np.sqrt(2), -1. / np.sqrt(2)]\n",
    "    ], dtype=torch.float32)\n",
    "    centers = scale * centers\n",
    "\n",
    "    x = torch.randn(n, 2)\n",
    "    x = 0.5 * x\n",
    "\n",
    "    center_ids = torch.randint(0, 8, (n,))\n",
    "    x = x + centers[center_ids]\n",
    "\n",
    "    x = x / 2 ** 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "# generates data of dimension 2\n",
    "data = gen_data(10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a62be700e3c11",
   "metadata": {},
   "source": [
    "### Forward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "701ffe4f521dde96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.060240Z",
     "start_time": "2025-03-09T16:41:29.058397Z"
    }
   },
   "outputs": [],
   "source": [
    "def jvp(f, x, v):\n",
    "    return torch.autograd.functional.jvp(\n",
    "        f, x, v,\n",
    "        create_graph=torch.is_grad_enabled()\n",
    "    )\n",
    "\n",
    "\n",
    "def t_dir(f, t):\n",
    "    return jvp(f, t, torch.ones_like(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af5e7adc5483e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.066589Z",
     "start_time": "2025-03-09T16:41:29.064453Z"
    }
   },
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def get_m_s(self, x: Tensor, t: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> tuple[tuple[Tensor, Tensor], tuple[Tensor, Tensor]]:\n",
    "        def f(t_in):\n",
    "            return self.get_m_s(x, t_in)\n",
    "\n",
    "        return t_dir(f, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca89ed17a1ab35c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.074011Z",
     "start_time": "2025-03-09T16:41:29.071185Z"
    }
   },
   "outputs": [],
   "source": [
    "class AffineTransformID(AffineTransform):\n",
    "    @staticmethod\n",
    "    def get_m_s(x, t):\n",
    "        m = x\n",
    "        s = torch.ones_like(x)\n",
    "        return m, s\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, t): # this is VDM. \n",
    "        m, s = AffineTransformID.get_m_s(x, t)\n",
    "\n",
    "        dm = torch.zeros_like(x)\n",
    "        ds = torch.zeros_like(x)\n",
    "\n",
    "        return (m, s), (dm, ds)\n",
    "\n",
    "\n",
    "class AffineTransformHalfNeural(AffineTransform):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(d, d)\n",
    "\n",
    "    def get_m_s(self, x, t):\n",
    "        #x_t = torch.cat([x, t], dim=1)\n",
    "        m = self.net(x)\n",
    "\n",
    "        #m = x + t * m\n",
    "        s = torch.ones_like(x)\n",
    "\n",
    "        return m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9dc94f03b8d5ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.080854Z",
     "start_time": "2025-03-09T16:41:29.078374Z"
    }
   },
   "outputs": [],
   "source": [
    "class Gamma(nn.Module, ABC):\n",
    "    @staticmethod\n",
    "    def alpha_2(g):\n",
    "        return torch.sigmoid(-g)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigma_2(g):\n",
    "        return torch.sigmoid(g)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_gamma(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, t: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        return t_dir(self.get_gamma, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d2407ebefe9fcf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.087510Z",
     "start_time": "2025-03-09T16:41:29.085433Z"
    }
   },
   "outputs": [],
   "source": [
    "class GammaLinear(Gamma):\n",
    "    @staticmethod\n",
    "    def get_gamma(t):\n",
    "        return -10 + 20 * t\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(t):\n",
    "        g = GammaLinear.get_gamma(t)\n",
    "        dg = torch.ones_like(t) * 20\n",
    "        return g, dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57b352085eead962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.095591Z",
     "start_time": "2025-03-09T16:41:29.091922Z"
    }
   },
   "outputs": [],
   "source": [
    "class PosLinear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        weight = self.sp(self.weight)\n",
    "        bias = self.bias\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "\n",
    "class GammaVDM(Gamma):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = PosLinear(1, 1)\n",
    "        self.fc2 = PosLinear(1, 1024)\n",
    "        self.fc3 = PosLinear(1024, 1)\n",
    "\n",
    "    def get_unnorm_gamma(self, x):\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        y = self.fc2(x)\n",
    "        y = torch.sigmoid(y)\n",
    "        y = self.fc3(y)\n",
    "\n",
    "        return x + y\n",
    "\n",
    "    def get_gamma(self, t):\n",
    "        x_0 = torch.zeros(1, 1)\n",
    "        x_1 = torch.ones(1, 1)\n",
    "        y_0 = torch.ones(1, 1) * (-10)\n",
    "        y_1 = torch.ones(1, 1) * 10 #flag\n",
    "        y_gap = y_1 - y_0\n",
    "\n",
    "        x_adj = torch.cat([x_0, x_1, t], dim=0)\n",
    "        y_adj = self.get_unnorm_gamma(x_adj)\n",
    "        yo_0, yo_1, yo = y_adj[:1], y_adj[1:2], y_adj[2:]\n",
    "\n",
    "        y = y_0 + (y_1 - y_0) * (yo - yo_0) / (yo_1 - yo_0)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ec69a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaMuLAN(Gamma):\n",
    "    #implement get_gamma and forward methods\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.gamma_shape = config.gamma_shape\n",
    "        self.n_features = config.seq_len * config.embedding_dim #what does this reflect? -> n flattend features \n",
    "        self.min_gamma = self.config.gamma_min\n",
    "        self.max_minus_min_gamma = self.config.gamma_max - self.config.gamma_min\n",
    "        self.grad_min_epsilon = 0.001\n",
    "\n",
    "\n",
    "\n",
    "        #self.l1 = nn.Linear(self.n_features, self.n_features)\n",
    "        #if I want the noise injection to not depend on the input, more like vdm, since input dependence will be injected through ndm function transformation, I can just make the input of\n",
    "        #l1 to be 1. which means the input is just the time t. That is not fully equal to MuLAN since there the a, b, d are dependent only on input not on t.\n",
    "        #that also mean I will still need to implement the get_gamma method, but not the forward method, we need tdir still. \n",
    "        self.l1 = nn.Linear(2, self.n_features)\n",
    "        self.l2 = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_a = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_b = nn.Linear(self.n_features, self.n_features)\n",
    "        self.l3_c = nn.Linear(self.n_features, self.n_features)\n",
    "\n",
    "    def _eval_polynomial(self, a, b, c, t):\n",
    "        # Polynomial evaluation\n",
    "        polynomial = (\n",
    "            (a ** 2) * (t ** 5) / 5.0\n",
    "            + (b ** 2 + 2 * a * c) * (t ** 3) / 3.0\n",
    "            + a * b * (t ** 4) / 2.0\n",
    "            + b * c * (t ** 2)\n",
    "            + (c ** 2 + self.grad_min_epsilon) * t)\n",
    "        \n",
    "        scale = ((a ** 2) / 5.0\n",
    "                 + (b ** 2 + 2 * a * c) / 3.0\n",
    "                 + a * b / 2.0\n",
    "                 + b * c\n",
    "                 + (c ** 2 + self.grad_min_epsilon))\n",
    "\n",
    "        return self.min_gamma + self.max_minus_min_gamma * polynomial / scale\n",
    "    \n",
    "    def _grad_t(self, a, b, c, t):\n",
    "        # derivative = (at^2 + bt + c)^2\n",
    "        polynomial = (\n",
    "        (a ** 2) * (t ** 4)\n",
    "        + (b ** 2 + 2 * a * c) * (t ** 2)\n",
    "        + a * b * (t ** 3) * 2.0\n",
    "        + b * c * t * 2\n",
    "        + (c ** 2 + self.grad_min_epsilon))\n",
    "        \n",
    "        scale = ((a ** 2) / 5.0\n",
    "                + (b ** 2 + 2 * a * c) / 3.0\n",
    "                + a * b / 2.0\n",
    "                + b * c\n",
    "                + (c ** 2 + self.grad_min_epsilon))\n",
    "\n",
    "        return self.max_minus_min_gamma * polynomial / scale\n",
    "\n",
    "    def _compute_coefficients(self, x):\n",
    "        _h = torch.nn.functional.silu(self.l1(x))\n",
    "        _h = torch.nn.functional.silu(self.l2(_h))\n",
    "        a = self.l3_a(_h)\n",
    "        b = self.l3_b(_h)\n",
    "        c = 1e-3 + torch.nn.functional.softplus(self.l3_c(_h))\n",
    "        #print(a,b,c)\n",
    "        return a, b, c\n",
    "    \n",
    "    def get_gamma(self, t, x):\n",
    "        a, b, c = self._compute_coefficients(x)\n",
    "        gamma_flat = self._eval_polynomial(a, b, c, t)\n",
    "        #shape should be bs=t.shape[0], gamma_shape\n",
    "        #how do I append a value to the shape though?\n",
    "        \n",
    "        gamma = gamma_flat.view(-1, *self.gamma_shape)\n",
    "        #print(gamma.shape, \"gamma shape\")\n",
    "        return gamma\n",
    "    \n",
    "    def forward(self, t, x):\n",
    "        a, b, c = self._compute_coefficients(x)\n",
    "        dg = self._grad_t(a, b, c, t)\n",
    "        dg = dg.clamp(min=self.grad_min_epsilon)\n",
    "        return self.get_gamma(t, x), dg\n",
    "\n",
    "    #def forward(self, t):\n",
    "    #    gamma, dgamma = t_dir(self.get_gamma, t)\n",
    "    #    #dgamma = torch.clamp(dgamma, min=self.grad_min_epsilon)\n",
    "    #    return gamma, dgamma\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def sigma_2(g):\n",
    "\n",
    "        vector = torch.sigmoid(g)\n",
    "        sigma_2 = torch.diagonal\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "mulan_config = SimpleNamespace(\n",
    "                gamma_shape=(1,),\n",
    "                seq_len= 1,\n",
    "                embedding_dim= 1,\n",
    "                gamma_min= -13.3,\n",
    "                gamma_max= 5.0\n",
    "                )\n",
    "\n",
    "gamma_here = GammaMuLAN(mulan_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bb1ba44e681c730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.101414Z",
     "start_time": "2025-03-09T16:41:29.099677Z"
    }
   },
   "outputs": [],
   "source": [
    "class VolatilityEta(nn.Module, ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, t: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2a2701951ed0b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.108064Z",
     "start_time": "2025-03-09T16:41:29.105751Z"
    }
   },
   "outputs": [],
   "source": [
    "class VolatilityEtaOne(nn.Module):\n",
    "    def forward(self, t):\n",
    "        return torch.ones_like(t)\n",
    "\n",
    "\n",
    "class VolatilityEtaNeural(nn.Module, ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(1, 1)\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.sp(self.net(t))\n",
    "\n",
    "class VolatilityEtaOneNew(nn.Module):\n",
    "    def forward(self, t):\n",
    "        return torch.ones(t.size(0), 2)\n",
    "#vol = VolatilityEtaOneNew()\n",
    "#print(vol(torch.tensor([1,2,4,5,6,7,54,2,3,4,5,6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab22518a1370b71",
   "metadata": {},
   "source": [
    "### Reverse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eabb9e1ff1eaabef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.114545Z",
     "start_time": "2025-03-09T16:41:29.112180Z"
    }
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = Net(d + 1, d)\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        z_t = torch.cat([z, t], dim=1)\n",
    "        x = self.net(z_t)\n",
    "\n",
    "        x = (1 - t) * z + (t + 0.01) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416548974191c4d",
   "metadata": {},
   "source": [
    "### Neural diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de00a7a5911f1cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.122222Z",
     "start_time": "2025-03-09T16:41:29.118767Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralDiffusion(nn.Module):\n",
    "    def __init__(self, transform: AffineTransform, gamma: Gamma, vol_eta: VolatilityEta, pred: Predictor, VAE=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.gamma = gamma\n",
    "        self.vol_eta = vol_eta\n",
    "        self.pred = pred\n",
    "        #self.scalar = nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "        self.VAE = VAE\n",
    "        if VAE:\n",
    "            self.model = Net(2, 4)\n",
    "            print(self.model.requires_grad_, \"model requires grad\")\n",
    "        else:\n",
    "            self.model = Net(2, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x: Tensor, t: Tensor):\n",
    "        #print(\"fakka\")\n",
    "        #print(torch.is_grad_enabled(), \"is grad enabled\")\n",
    "        eps = torch.randn_like(x)\n",
    "\n",
    "        # Check if eps contains NaNs or Infs\n",
    "        if torch.any(torch.isnan(eps)) or torch.any(torch.isinf(eps)):\n",
    "            print(f\"NaN or Inf detected in eps: {eps}\")\n",
    "\n",
    "        if self.VAE:\n",
    "            mean, log_std = self.model(x).chunk(2, dim=1)\n",
    "            std = torch.exp(log_std)\n",
    "            context = mean + std * torch.randn_like(std)\n",
    "        else:\n",
    "            context = self.model(x)\n",
    "\n",
    "        gamma, d_gamma = self.gamma(t, context)\n",
    "\n",
    "        # Check gamma and d_gamma for NaNs and Infs\n",
    "        if torch.any(torch.isnan(gamma)) or torch.any(torch.isinf(gamma)):\n",
    "            print(f\"NaN or Inf detected in gamma: {gamma}\")\n",
    "            print(gamma[torch.isinf(gamma)])\n",
    "            print(gamma[torch.isnan(gamma)])\n",
    "\n",
    "        if torch.any(torch.isnan(d_gamma)) or torch.any(torch.isinf(d_gamma)): # or torch.any(d_gamma < 0.000001):\n",
    "            print(f\"NaN or Inf detected in d_gamma: {d_gamma}\")\n",
    "        \n",
    "        sigma_2 = self.gamma.sigma_2(gamma)\n",
    "        alpha_2 = self.gamma.alpha_2(gamma)\n",
    "        alpha = alpha_2 ** 0.5\n",
    "        sigma = sigma_2 ** 0.5\n",
    "\n",
    "        # Check alpha and sigma for NaNs or Infs\n",
    "        if torch.any(torch.isnan(alpha)) or torch.any(torch.isinf(alpha)):\n",
    "            print(f\"NaN or Inf detected in alpha: {alpha}\")\n",
    "        if torch.any(torch.isnan(sigma)) or torch.any(torch.isinf(sigma)):\n",
    "            print(f\"NaN or Inf detected in sigma: {sigma}\")\n",
    "\n",
    "        (m, _), (d_m, _) = self.transform(x, t)\n",
    "\n",
    "        # Check m, d_m for NaNs or Infs\n",
    "        if torch.any(torch.isnan(m)) or torch.any(torch.isinf(m)):\n",
    "            print(f\"NaN or Inf detected in m: {m}\")\n",
    "        if torch.any(torch.isnan(d_m)) or torch.any(torch.isinf(d_m)):\n",
    "            print(f\"NaN or Inf detected in d_m: {d_m}\")\n",
    "\n",
    "        eta = self.vol_eta(t)\n",
    "\n",
    "        # Check eta for NaNs or Infs\n",
    "        if torch.any(torch.isnan(eta)) or torch.any(torch.isinf(eta)):\n",
    "            print(f\"NaN or Inf detected in eta: {eta}\")\n",
    "\n",
    "        z = alpha * m + sigma * eps\n",
    "\n",
    "        # Check z for NaNs or Infs\n",
    "        if torch.any(torch.isnan(z)) or torch.any(torch.isinf(z)):\n",
    "            print(f\"NaN or Inf detected in z: {z}\")\n",
    "\n",
    "        x_ = self.pred(z, t)\n",
    "        #print(x_.requires_grad, \"x_ requires grad\")\n",
    "\n",
    "        # Check x_ for NaNs or Infs\n",
    "        if torch.any(torch.isnan(x_)) or torch.any(torch.isinf(x_)):\n",
    "            print(f\"NaN or Inf detected in x_: {x_}\")\n",
    "\n",
    "        (m_, _), (d_m_, _) = self.transform(x_, t)\n",
    "\n",
    "        # Check m_, d_m_ for NaNs or Infs\n",
    "        if torch.any(torch.isnan(m_)) or torch.any(torch.isinf(m_)):\n",
    "            print(f\"NaN or Inf detected in m_: {m_}\")\n",
    "        if torch.any(torch.isnan(d_m_)) or torch.any(torch.isinf(d_m_)):\n",
    "            print(f\"NaN or Inf detected in d_m_: {d_m_}\")\n",
    "\n",
    "        # ELBO weighting\n",
    "        lmbd = 0.5 * torch.exp(-gamma) * d_gamma / eta\n",
    "        #lmbd = 0.5 * torch.exp(-self.scalar * gamma) * d_gamma / eta\n",
    "        #lmbd = 0.5 * 8 / eta\n",
    "        #print(lmbd)\n",
    "\n",
    "        \n",
    "        #lmbd = 0.5 * d_gamma / eta\n",
    "        #print(\"exp gamma is : \", torch.exp(-gamma))\n",
    "        #print(\"d_gamma is : \", d_gamma)\n",
    "        if self.VAE:\n",
    "            # Check lmbd for NaNs or Infs\n",
    "            if torch.any(torch.isnan(lmbd)) or torch.any(torch.isinf(lmbd)):\n",
    "                print(f\"NaN or Inf detected in lmbd: {lmbd}\")\n",
    "\n",
    "            # L_x weighting (optional, commented out)\n",
    "            #lmbd = (4 / (1 + eta) ** 2) \n",
    "            #print(\"lmbd is: \", lmbd) #should just be 1, it is\n",
    "            one_over_dgamma = torch.clamp(1 / (d_gamma), max=10000) \n",
    "            if torch.any(one_over_dgamma == 10000):\n",
    "                print(\"clamped\")\n",
    "            \n",
    "            #print(\"one_over_dgamma is: \", one_over_dgamma) #should just be 1, it is\n",
    "\n",
    "            loss = (1 + eta) / 2 * (m - m_) + one_over_dgamma * (d_m - d_m_)\n",
    "\n",
    "            #print(d_gamma[d_gamma < 0.000001], \"dgamma\")\n",
    "            #print(one_over_dgamma[d_gamma < 0.000001])\n",
    "            #print(d_m-d_m_, \"should be zero\")\n",
    "            #print(one_over_dgamma*(d_m - d_m_), \"should be zero\")\n",
    "        \n",
    "            #print(\"unscaled loss is: \", loss)\n",
    "\n",
    "            # Check intermediate loss for NaNs or Infs\n",
    "            if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "                print(f\"NaN or Inf detected in loss before squaring: {loss}\")\n",
    "\n",
    "            loss = loss ** 2\n",
    "\n",
    "            # Check squared loss for NaNs or Infs\n",
    "            if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "                print(f\"NaN or Inf detected in squared loss: {loss}\")\n",
    "\n",
    "            # Stabilises training\n",
    "            #loss_x = (1 + eta) ** 2 / 4 * (x - x_) ** 2\n",
    "\n",
    "            # Check loss_x for NaNs or Infs\n",
    "            #if torch.any(torch.isnan(loss_x)) or torch.any(torch.isinf(loss_x)):\n",
    "            #    print(f\"NaN or Inf detected in loss_x: {loss_x}\")\n",
    "\n",
    "            #loss = 0.5 * loss + 0.5 * loss_x\n",
    "\n",
    "            # Check final loss before applying lambda for NaNs or Infs\n",
    "            if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "                print(f\"NaN or Inf detected in final loss before lambda: {loss}\")\n",
    "\n",
    "            loss = lmbd * loss \n",
    "\n",
    "            # Check final loss after applying lambda for NaNs or Infs\n",
    "            if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "                print(f\"NaN or Inf detected in final loss: {loss}\")\n",
    "\n",
    "            # Optionally, you can add assertions here to ensure that no NaN or Inf values propagate.\n",
    "            assert not torch.any(torch.isnan(loss)), f\"NaN detected in final loss: {loss}\"\n",
    "            assert not torch.any(torch.isinf(loss)), f\"Inf detected in final loss: {loss}\"\n",
    "\n",
    "            loss = loss.sum(dim=1)\n",
    "            #print(loss.shape)\n",
    "\n",
    "            # Check final loss sum for NaNs or Infs\n",
    "            if torch.any(torch.isnan(loss)) or torch.any(torch.isinf(loss)):\n",
    "                print(f\"NaN or Inf detected in loss after summing: {loss}\")\n",
    "\n",
    "            \n",
    "            #if we are using VAE we need to also compute the prior VAE loss \n",
    "            if self.VAE:\n",
    "                #print(log_std.shape)\n",
    "                #print(mean.shape)\n",
    "                KLD = -0.5 * torch.sum(1 + (2*log_std) - mean**2 - (2*log_std).exp(), dim=-1)\n",
    "                #print(KLD.shape)\n",
    "                #print(loss.shape)\n",
    "                #print(KLD.requires_grad, \"KLD requires grad\")\n",
    "\n",
    "            #check KLD for NaNs or Infs\n",
    "                if torch.any(torch.isnan(KLD)) or torch.any(torch.isinf(KLD)):\n",
    "                    print(f\"NaN or Inf detected in KLD: {KLD}\")\n",
    "                    print(KLD)\n",
    "                    print(KLD[torch.isinf(KLD)])\n",
    "                    print(KLD[torch.isnan(KLD)])\n",
    "                    print(KLD[KLD > 10000])\n",
    "            \n",
    "            #print(loss.mean(), \"loss mean\")\n",
    "                loss = loss + KLD\n",
    "        else:\n",
    "            #compute loss for the x_pred case?\n",
    "            gamma_, d_gamma_ = self.gamma(t, x_)\n",
    "            alpha_2_ = self.gamma.alpha_2(gamma_)\n",
    "            sigma_2_ = self.gamma.sigma_2(gamma_)\n",
    "            sigma_ = sigma_2 ** 0.5\n",
    "\n",
    "            f_b = - 0.5 * d_gamma * ( (1 - alpha_2) * alpha_2 - (z - m) * (eta + sigma * (1 - sigma_2)))\n",
    "            f_b_ = - 0.5 * d_gamma_ * ( (1 - alpha_2_) * alpha_2_ - (z - m_) * (eta + sigma_ * (1 - sigma_2_)))\n",
    "            loss = 1/(2 * sigma_2 * d_gamma * eta) * (f_b - f_b_) ** 2\n",
    "            #print(loss.shape)\n",
    "            loss = loss.sum(dim=1)\n",
    "            #print(loss.shape)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bae11e352252f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f391274f4f43b1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.261219Z",
     "start_time": "2025-03-09T16:41:29.126692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_sample = gen_data(2 ** 12)\n",
    "\n",
    "viz_2d_data(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d977274d1d3cb88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:41:29.270692Z",
     "start_time": "2025-03-09T16:41:29.267685Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, time_sampler, gen_data_f):\n",
    "    iter = 10 ** 5\n",
    "    bs = 2 ** 10\n",
    "\n",
    "    #set lr higher for model.gamma than for the rest using param group\n",
    "\n",
    "    optim = torch.optim.Adam([*model.parameters(), *time_sampler.parameters()], lr=1e-4) #e-3 for vae way\n",
    "\n",
    "    pbar = trange(iter)\n",
    "    for i in pbar:\n",
    "        x = gen_data_f(bs)\n",
    "\n",
    "        t, p = time_sampler(bs=bs)\n",
    "        t, p = t.detach(), p.detach()\n",
    "\n",
    "        loss = model(x, t)\n",
    "        loss = loss / p + time_sampler.loss(loss.detach(), t)\n",
    "\n",
    "        # plt.scatter(t[:, 0], loss.detach().numpy(), s=1)\n",
    "        # plt.scatter(t[:, 0], p.detach().numpy(), s=1)\n",
    "        # plt.show()\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            pbar.set_description(f\"{loss.item():.4f}\")\n",
    "\n",
    "        #print(loss)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770bbf8afcc9da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:36.632794Z",
     "start_time": "2025-03-09T16:41:29.282810Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:00<8:32:29,  3.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483.2229:   1%|▏         | 1427/100000 [01:04<2:49:12,  9.71it/s]"
     ]
    }
   ],
   "source": [
    "mulan_config = SimpleNamespace(\n",
    "                gamma_shape=(1,),\n",
    "                seq_len= 1,\n",
    "                embedding_dim= 1,#2\n",
    "                gamma_min= -10, #-13.3\n",
    "                gamma_max= 10, # 5\n",
    "                learn_tau=False,\n",
    "                learn_delta=False\n",
    "            )\n",
    "torch.set_grad_enabled(True)           \n",
    "transform = AffineTransformID()\n",
    "#transform = AffineTransformHalfNeural(d=2)\n",
    "\n",
    "#gamma = GammaLinear()\n",
    "#gamma = GammaVDM()\n",
    "#gamma = GammaBad()\n",
    "gamma = GammaMuLAN(mulan_config)\n",
    "\n",
    "\n",
    "vol_eta = VolatilityEtaOne()\n",
    "#vol_eta = VolatilityEtaNeural()\n",
    "\n",
    "pred = Predictor(d=2)\n",
    "\n",
    "ndm = NeuralDiffusion(transform, gamma, vol_eta, pred, VAE=False)#, VAE=True)\n",
    "ndm.train()\n",
    "\n",
    "time_sampler = UniformBucketSampler()\n",
    "#time_sampler = UniformSampler()\n",
    "\n",
    "train(model=ndm, time_sampler=time_sampler, gen_data_f=gen_data)\n",
    "print(\"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distibution of the VAE latent space\n",
    "def plot_vae_latent_space(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, log_std = model.model(data).chunk(2, dim=1)\n",
    "        std = torch.exp(log_std)\n",
    "        context = mean + std * torch.randn_like(std)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(context[:, 0].numpy(), context[:, 1].numpy(), s=1)\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.title(\"VAE Latent Space\")\n",
    "    plt.show()\n",
    "\n",
    "plot_vae_latent_space(ndm, gen_data(2 ** 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(sampler: BucketSampler, num_points=1000):\n",
    "    with torch.no_grad():\n",
    "        # Generate evenly spaced t values\n",
    "        t_values = torch.linspace(0, 1, num_points).view(-1, 1)  # Shape: (num_points, 1)\n",
    "        \n",
    "        # Compute p(t) at these points\n",
    "        p_values = sampler.prob(t_values).cpu().numpy().flatten()\n",
    "        t_values = t_values.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute the integral using the trapezoidal rule\n",
    "        dt = 1 / (num_points - 1)  # Step size\n",
    "        integral = np.trapz(p_values, t_values)  # Numerical integration\n",
    "\n",
    "        # Plot the learned PDF\n",
    "        plt.plot(t_values, p_values, label=f\"Learned PDF (∫p(t)dt ≈ {integral:.4f})\", color=\"b\", linewidth=2)\n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"p(t)\")\n",
    "        plt.title(\"Learned Probability Density Function\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Print the integral value\n",
    "        print(f\"Estimated integral of p(t) over [0,1]: {integral:.4f}\")\n",
    "        if abs(integral - 1) > 0.05:\n",
    "            print(\"⚠️ Warning: PDF is not properly normalized!\")\n",
    "        else:\n",
    "            print(\"✅ PDF is properly normalized.\")\n",
    "\n",
    "plot_pdf(time_sampler)\n",
    "\n",
    "#isnt this the wrong way arround? loss should be higher for high t not low t, and gamma isnt focussed on any particular part yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40946a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    gamma_og = GammaMuLAN(mulan_config)\n",
    "    x = gen_data(300)\n",
    "\n",
    "    #g_og, d_gamma = gamma_og(t, x)\n",
    "\n",
    "    g, _ = gamma(t, x)\n",
    "\n",
    "\n",
    "    plt.plot(t, g)\n",
    "    #plt.plot(t, g_og)\n",
    "    plt.legend([\"Learned\", \"Original\"])\n",
    "    plt.show()\n",
    "\n",
    "    #print(d_gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also print the loss for these same values of 10\n",
    "#x = gen_data(1).repeat(1000, 1)\n",
    "x = gen_data(1000)\n",
    "t_points = torch.linspace(0, 1, 1000)[:, None]\n",
    "\n",
    "losses = []\n",
    "for t in t_points:\n",
    "    t = t.expand(x.shape[0], 1)\n",
    "    #print(t.shape)\n",
    "    #print(x.shape)\n",
    "    #print(t)\n",
    "    loss = ndm(x, t)\n",
    "    avg_loss = loss.mean()\n",
    "    losses.append(avg_loss.item())\n",
    "\n",
    "\n",
    "print(losses)\n",
    "loss = torch.tensor(losses)\n",
    "plt.plot(t_points, loss.detach().numpy())\n",
    "plt.show() \n",
    "\n",
    "#same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1dec6",
   "metadata": {},
   "source": [
    "### Train MuLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74017b13",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "f^B = \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) - \\frac{g^2}{2} \\frac{\\alpha F - z}{\\sigma^2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8700620e4e064",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "### SDE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8a518fd897855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:50.212269Z",
     "start_time": "2025-03-09T16:47:45.548537Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "\n",
    "def sde(z_in, t_in):\n",
    "    \n",
    "    x_ = pred(z_in, t_in)\n",
    "    if ndm.VAE:\n",
    "        context = torch.randn_like(z_in)\n",
    "    else:\n",
    "        context = ndm.model(x_)\n",
    "    \n",
    "    gmm, d_gmm = gamma(t_in, context)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5 #I added this \n",
    "    if torch.any(torch.isnan(gmm)) or torch.any(torch.isinf(gmm)):\n",
    "        print(t_in, \"t_in\")\n",
    "        print(x_, \"x_\")\n",
    "        print(gmm[torch.isnan(gmm)], \"gmm\")\n",
    "        print(\"gmm is nan or inf\")\n",
    "    #print(gmm, \"gmm\")\n",
    "    #print(d_gmm, \"d_gmm\")\n",
    "    #print(alpha_2, \"alpha_2\")\n",
    "    #print(sigma_2, \"sigma_2\")\n",
    "    #print(alpha, \"alpha\")\n",
    "    #print(sigma, \"sigma\")\n",
    "\n",
    "    eta = vol_eta(t_in)\n",
    "\n",
    "    g = (sigma_2 * d_gmm * eta) ** 0.5\n",
    "\n",
    "    (m_, _), (d_m_, _) = transform(x_, t_in)\n",
    "\n",
    "    #print(d_m_, \"d_m\")\n",
    "    #print(sigma_2, \"sigma2\")\n",
    "    #print(eta, \"eta\")\n",
    "    #print(d_gmm, \"d_gmm\")\n",
    "    #print(g, \"g\")\n",
    "\n",
    "    #drift = -alpha * d_gmm * (1 + eta) / 2 * m_ + \\\n",
    "    #        alpha * d_m_ + \\\n",
    "    #        0.5 * d_gmm * (alpha_2 + eta) * z_in\n",
    "    eps = (z_in - alpha * m_) / sigma\n",
    "    alpha_prime = - d_gmm * 0.5 * alpha * (1- alpha_2) \n",
    "    sigma_prime = 0.5 * d_gmm * sigma * (1 - sigma_2)\n",
    "    #dz = -alpha * d_gmm + alpha * d_m_ + sigma * d_gmm * eps\n",
    "    dz = alpha_prime * m_ + alpha * d_m_ + sigma_prime * eps\n",
    "    drift = dz - 0.5 * (g ** 2) * ((alpha * m_ - z_in) / sigma_2)\n",
    "\n",
    "    #print(eps, \"eps\")\n",
    "    #print(alpha_prime, \"alpha_prime\")\n",
    "    #print(sigma_prime, \"sigma_prime\")\n",
    "    #print(dz, \"dz\")\n",
    "    #print(drift, \"drift\")\n",
    "    if torch.any(torch.isnan(drift)) or torch.any(torch.isinf(drift)):\n",
    "        print(\"drift is nan or inf\")\n",
    "    \n",
    "    if torch.any(torch.isnan(g)) or torch.any(torch.isinf(g)):\n",
    "        print(\"g is nan or inf\")\n",
    "\n",
    "    return drift, g\n",
    "\n",
    "\n",
    "_, (t_steps, path) = solve_sde(sde=sde, z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5ebdf",
   "metadata": {},
   "source": [
    "### Star Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e332ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoised_fn(x, t):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7864187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting_part(prev_sample, t, denoised_fn):\n",
    "    def process_xstart(x):\n",
    "        if denoised_fn is not None:\n",
    "            # print(denoised_fn)\n",
    "            x = denoised_fn(x, t)\n",
    "        if False:# clip_denoised:\n",
    "            return x.clamp(-1, 1)\n",
    "        return x\n",
    "    \n",
    "    out = {}\n",
    "    x_ = pred(prev_sample, t) \n",
    "    \n",
    "    x_start = process_xstart(x_)\n",
    "    out[\"pred_xstart\"] = x_start\n",
    "\n",
    "    gmm, _ = gamma(t)\n",
    "    alpha = gamma.alpha_2(gmm) ** 0.5\n",
    "    sigma2 =  gamma.sigma_2(gmm)\n",
    "\n",
    "    m, _ = transform.get_m_s(x_start, t)\n",
    "\n",
    "    out[\"mean\"] = alpha*m\n",
    "    out[\"log_variance\"] = torch.log(sigma2)\n",
    "\n",
    "\n",
    "    noise = torch.randn_like(prev_sample)\n",
    "    nonzero_mask = (\n",
    "        (t != 0).float().view(-1, *([1] * (len(prev_sample.shape) - 1)))\n",
    "    )  # no noise when t == 0\n",
    "    sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise \n",
    "    return sample\n",
    "\n",
    "@torch.no_grad()\n",
    "def discrete_sampling_star(\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=True\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)#[:-1]\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        z = predicting_part(prev_sample=z, t=t, denoised_fn=None)\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path[:-1]))\n",
    "\n",
    "\n",
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "_, (t_steps, path) = discrete_sampling_star(z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390c795",
   "metadata": {},
   "source": [
    "### Correct Marginal Sampling\n",
    "\n",
    "\\begin{align}\n",
    "    z_t = F(\\varepsilon, t, x)\n",
    "\\end{align}\n",
    "\n",
    "Then given z_t and t (n(0,1) noise and t=1), do:\n",
    "\n",
    "1. x_0 prediction, $$x\\_ = model.pred(z_t, t)$$\n",
    "2. get epsilon by inverse big F, $$\\varepsilon = \\frac{(z - \\alpha F) }{\\sigma}$$\n",
    "3. get epsilon s|t, \n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\varepsilon}_{s|t} = \\sqrt{1- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\sigma}_{s|t} = \\sigma_s^2 - \\frac{SNR(t)}{SNR(s)}\\sigma_s^2\n",
    "\\end{align}\n",
    "\n",
    "and,\n",
    "\n",
    "\\begin{align}\n",
    " \\tilde{\\varepsilon} - N(0,1)? \n",
    "\\end{align}\n",
    "because otherwise no new noise would be injected\n",
    "\n",
    "\n",
    "4. get z_s (next z) by feeding through the forward process. \n",
    "\\begin{align}\n",
    "    z = F(\\tilde{\\varepsilon}_{s|t}, s, x)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha_s F + \\sigma_s (\\sqrt{1- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n",
    "but that doesnt match the ndm appendix 1 I think? and it doesnt lead to a marginalized standard deviation of sigma_s, which would instead need,\n",
    "\n",
    "\\begin{align}\n",
    "    z = \\alpha_s F + (\\sqrt{\\sigma_s^2 - \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n",
    "which I suppose you could also write as \n",
    "\\begin{align}\n",
    "    \\tilde{\\varepsilon}_{s|t} = \\frac{1}{\\sigma^2_s}(\\sqrt{\\sigma^2_s- \\tilde{\\sigma}^2_{s|t}} \\varepsilon + \\tilde{\\sigma}_{s|t} \\tilde{\\varepsilon})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_marginal(prev_sample, t, s, denoised_fn):\n",
    "    def process_xstart(x):\n",
    "        if denoised_fn is not None:\n",
    "            # print(denoised_fn)\n",
    "            x = denoised_fn(x, t)\n",
    "        if False:# clip_denoised:\n",
    "            return x.clamp(-1, 1)\n",
    "        return x\n",
    "    \n",
    "    #step 1 do prediction \n",
    "    x_ = pred(prev_sample, t) \n",
    "    \n",
    "    x_start = process_xstart(x_)\n",
    "\n",
    "    #step 2 get epsilon\n",
    "    gmm, _ = gamma(t)\n",
    "    alpha2 = gamma.alpha_2(gmm)\n",
    "    sigma2 =  gamma.sigma_2(gmm)\n",
    "    alpha = alpha2 ** 0.5\n",
    "    sigma = sigma2 ** 0.5\n",
    "\n",
    "    m_ , _ = transform.get_m_s(x_start, t)\n",
    "\n",
    "    eps = (prev_sample - alpha * m_) / sigma\n",
    "\n",
    "    #step 3 get epsilon s|t\n",
    "    #we need stepsize for this?\n",
    "    noise = torch.randn_like(prev_sample)\n",
    "    gmm_s, _ = gamma(s)\n",
    "    alpha2_s = gamma.alpha_2(gmm_s)\n",
    "    sigma2_s =  gamma.sigma_2(gmm_s)\n",
    "    alpha_s = alpha2_s ** 0.5\n",
    "    sigma_s = sigma2_s ** 0.5\n",
    "    \n",
    "    m_s , _ = transform.get_m_s(x_start, s)\n",
    "\n",
    "    #print(gmm_s)\n",
    "    snr_t = (alpha2/sigma2).double()\n",
    "    snr_s = (alpha2_s/sigma2_s).double()\n",
    "\n",
    "    #sigma2_tilde_s_t = (1 -  (snr_t / snr_s)).float() #instead of casting back to float here we can alos cast back only after computing epsilon tilde st\n",
    "    sigma2_tilde_s_t = -torch.expm1(gmm_s - gmm) #should be in 0-1\n",
    "    #print(sigma2_tilde_s_t, \"sigma2_tilde_s_t\")\n",
    "\n",
    "    #or option 3\n",
    "\n",
    "    epsilon_tilde_s_t = torch.sqrt(1 - sigma2_tilde_s_t) * eps + (sigma2_tilde_s_t.sqrt()) * noise \n",
    "\n",
    "    #print(\"snr_t\", snr_t[0])\n",
    "    #print(\"snr_s\", snr_s[0])\n",
    "    #print(\"sigma\", sigma2_tilde_s_t) #this should be positive always but isnt so im doing something wrong. \n",
    "\n",
    "    #step 4 get z_s\n",
    "    sample = alpha_s * m_s + sigma_s * epsilon_tilde_s_t\n",
    "    \n",
    "    #if we want to match appendix 1 of ndm paper I think it should instead be\n",
    "    #sample = alpha_s * m_s +  torch.sqrt(sigma2 - sigma2_tilde_s_t) * eps + (sigma2_tilde_s_t ** 0.5) * noise\n",
    "\n",
    "    return sample\n",
    "\n",
    "@torch.no_grad()\n",
    "def discrete_sampling(\n",
    "        z: Tensor,\n",
    "        ts: float,\n",
    "        tf: float,\n",
    "        n_steps: int,\n",
    "        show_pbar: bool=True\n",
    "):\n",
    "    bs = z.shape[0]\n",
    "\n",
    "    t_steps = torch.linspace(ts, tf, n_steps + 1)#[:-1]\n",
    "    dt = (tf - ts) / n_steps\n",
    "    dt_2 = abs(dt) ** 0.5\n",
    "\n",
    "    path = [z]\n",
    "    pbar = tqdm if show_pbar else (lambda a: a)\n",
    "    for t in pbar(t_steps[:-1]):\n",
    "        t = t.expand(bs, 1)\n",
    "\n",
    "        #I understand I am doing 2x-1 the number of needed forward pass through gamma now, Ill fix that before putting it into the actual code.        \n",
    "        z = get_next_marginal(prev_sample=z, t=t, s=t+dt, denoised_fn=None)\n",
    "\n",
    "        path.append(z)\n",
    "\n",
    "    return z, (t_steps, torch.stack(path))\n",
    "\n",
    "\n",
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "_, (t_steps, path) = discrete_sampling(z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "print(path[-1])\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b811cd",
   "metadata": {},
   "source": [
    "### ODE sampling\n",
    "\n",
    "that means we need just f(z,t,eps)|eps=F(z,t,)înverser. We have, \\\\\n",
    "\\begin{align}\n",
    "    z = F(\\varepsilon, t, x),\n",
    "\\end{align}\n",
    "and specifically in the ndm/vdm case, \\\\\n",
    "\\begin{align}\n",
    "    z = \\alpha F(x, t) + \\sigma \\varepsilon\n",
    "\\end{align}\n",
    "The ODE drift is given by \\\\\n",
    "\\begin{align}\n",
    "    dz = f_{tilde}(z_t, t, x\\_)dt\n",
    "\\end{align}\n",
    "Where \n",
    "\\begin{align}\n",
    "    f_{tilde}(z_t, t, x\\_) &= d/dt F(eps, t, x) \\\\\n",
    "    &= alpha' * m + alpha * dm + sigma' * eps\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "f &= \\dot{\\alpha} F + \\alpha \\dot{F} + \\frac{\\dot{\\sigma}}{\\sigma} (z - \\alpha F) \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e16e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2 ** 12\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "\n",
    "def ode(z_in, t_in):\n",
    "    gmm, d_gmm = gamma(t_in)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5\n",
    "\n",
    "    #eta = vol_eta(t_in)\n",
    "\n",
    "    x_ = pred(z_in, t_in)\n",
    "\n",
    "    (m_, _), (d_m_, _) = transform(x_, t_in)\n",
    "\n",
    "    eps = (z_in - alpha * m_) / sigma\n",
    "    alpha_prime = - d_gmm * 0.5 * alpha * (1- alpha_2) \n",
    "    sigma_prime = 0.5 * d_gmm * sigma * (1 - sigma_2)\n",
    "    #dz = -alpha * d_gmm + alpha * d_m_ + sigma * d_gmm * eps\n",
    "    dz = alpha_prime * m_ + alpha * d_m_ + sigma_prime * eps\n",
    "    #dz = -alpha_prime + alpha * d_m_ + sigma_prime * eps\n",
    "\n",
    "    \n",
    "    return dz, 0\n",
    "\n",
    "\n",
    "_, (t_steps, path) = solve_sde(sde=ode, z=z, ts=1, tf=0, n_steps=300)\n",
    "\n",
    "viz_2d_path(t_steps, path, n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a852b",
   "metadata": {},
   "source": [
    "### Adaptive step size ODE sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "def ode_adaptor(t, z):\n",
    "    #print(t)\n",
    "    t = t.expand(z.shape[0], 1)\n",
    "    #print(z.shape)\n",
    "    #print(t.shape) # bs x 2, why is this bs by 2??, needs to be bs x 1 where t moves from 1 to 0 over time\n",
    "    drift, _ = ode(z, t)\n",
    "    return drift\n",
    "\n",
    "@torch.no_grad()\n",
    "def integrate_ode_torch(z0, t_span=(1.0, 0.0), atol=1e-6, rtol=1e-7):\n",
    "    # Here, we only provide the initial conditions (z0) and the time span (t_span)\n",
    "    # The solver will automatically select adaptive time steps within the provided range\n",
    "    t = torch.linspace(1.0, 0.0, 10000)  # Reverse time to go from 1 to 0\n",
    "    # Solve the ODE using RK45 (dopri5 with adaptive step size)\n",
    "    path = odeint(ode_adaptor, z0, t = t, method='dopri5', atol=atol, rtol=rtol)\n",
    "\n",
    "     # The solver automatically handles the time steps\n",
    "    final_state = path[-1]  # Last step of integration\n",
    "\n",
    "    return final_state, (t, path)\n",
    "\n",
    "bs = 2 ** 12\n",
    "print(bs)\n",
    "\n",
    "z = torch.randn(bs, 2)\n",
    "\n",
    "final, (t_steps, path) = integrate_ode_torch(z)\n",
    "\n",
    "print(\"steps taken\", len(path))\n",
    "print(\"steps\", t_steps)\n",
    "\n",
    "viz_2d_data(final.detach().numpy())\n",
    "viz_2d_path(t_steps.detach().numpy(), path.detach().numpy(), n_lines=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b43e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    g, _ = gamma(t)\n",
    "    alpha2 = gamma.alpha_2(g)\n",
    "    sigma2 = gamma.sigma_2(g)\n",
    "    snr = alpha2 / sigma2\n",
    "\n",
    "    plt.plot(t, snr)\n",
    "    plt.show()\n",
    "\n",
    "#clearly t > s implies snr(t) < snr(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf01e04ee671537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:47:50.335562Z",
     "start_time": "2025-03-09T16:47:50.236564Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "\n",
    "    g, _ = gamma(t)\n",
    "\n",
    "    plt.plot(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a63e0d4787360",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, 300)[:, None]\n",
    "    gmm, dgamma = gamma(t)\n",
    "    alpha_2 = gamma.alpha_2(gmm)\n",
    "    sigma_2 = gamma.sigma_2(gmm)\n",
    "    alpha = alpha_2 ** 0.5\n",
    "    sigma = sigma_2 ** 0.5\n",
    "\n",
    "    #also get alpha and sigma from the sqrt function and plot them in the same graph as the other alpha and sigma, so make three plots\n",
    "\n",
    "    plt.plot(alpha)\n",
    "    plt.legend([\"alpha\", \"alpha_sqrt\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(sigma) #flag2\n",
    "    plt.legend([\"sigma\", \"sigma_sqrt\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(gmm)\n",
    "    plt.legend([\"gamma\"])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(dgamma)\n",
    "    print(dgamma)\n",
    "    plt.legend([\"dgamma\"])\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
